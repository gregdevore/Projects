{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "# Lesson 1b: Basic Neural Network with Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "In this lesson you will build a small neural network in Keras and train it to replicate the logical XOR function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from IPython.display import SVG\n",
    "\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Input, Dense\n",
    "from keras.regularizers import l2\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "## Create dataset for the logical XOR function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "data_X = np.array([[0, 0],\n",
    "                   [1, 0],\n",
    "                   [0, 1],\n",
    "                   [1, 1]])\n",
    "data_y = np.array([0, \n",
    "                   1, \n",
    "                   1, \n",
    "                   0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "## Build the neural net model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(2, input_shape=(2,), activation='sigmoid', kernel_initializer='glorot_normal', \n",
    "                kernel_regularizer=l2(0.0001)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 2)                 6         \n",
      "=================================================================\n",
      "Total params: 6\n",
      "Trainable params: 6\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "# Question 1: \n",
    "How many parameters are there in the model so far? Why? Explain in detail what each parameter represents. Answer in the cell below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are six parameters in the model so far. There are six because we have two input nodes (one for each column of X), and a single layer with two nodes. The six parameters represent the following:\n",
    "* One weight parameter between each of the input nodes and each node in the first layer. Since there are two input nodes and two nodes in the first layer, there are a total of four parameters here.\n",
    "* One bias term for each of the nodes in the first layer. Since there are two nodes, there are a total of two bias terms here.\n",
    "\n",
    "The above adds up to a total of six parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "## Add another layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "model.add(Dense(1, activation='sigmoid', kernel_initializer='glorot_normal'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 2)                 6         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 3         \n",
      "=================================================================\n",
      "Total params: 9\n",
      "Trainable params: 9\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "## Question 2: \n",
    "How many new parameters are there now (e.g., how many were added after question 1)? Why? What does each new parameter represent? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three additional parameters were added after question one. This is because another layer was added, this time with only a single node. The new parameters represent the following:\n",
    "* One weight parameter between each of the two nodes in the first layer and the single node in the second layer. There are a total of two parameters here.\n",
    "* One bias term for the single node in the second layer.\n",
    "\n",
    "The above adds up to a toal of three new parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are now a total of 9 parameters:\n",
    "* One weight between each of the two input nodes and the two nodes in the first layer. This is a total of four parameters.\n",
    "* One bias term for each of the two nodes in the first layer. This adds two parameters and the running total is now six parameters.\n",
    "* One weight between each of the two nodes in the first layer and the single node in the second layer. This adds two parameters and the running total is now 8 parameters.\n",
    "* One bias term for the single node in the second layer. This adds one parameter and the grand total is 9 parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "## Visualize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"221pt\" viewBox=\"0.00 0.00 295.00 221.00\" width=\"295pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 217)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"white\" points=\"-4,4 -4,-217 291,-217 291,4 -4,4\" stroke=\"none\"/>\n",
       "<!-- 139813355699336 -->\n",
       "<g class=\"node\" id=\"node1\"><title>139813355699336</title>\n",
       "<polygon fill=\"none\" points=\"0,-166.5 0,-212.5 287,-212.5 287,-166.5 0,-166.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"81.5\" y=\"-185.8\">dense_1_input: InputLayer</text>\n",
       "<polyline fill=\"none\" points=\"163,-166.5 163,-212.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"190.5\" y=\"-197.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"163,-189.5 218,-189.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"190.5\" y=\"-174.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"218,-166.5 218,-212.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"252.5\" y=\"-197.3\">(None, 2)</text>\n",
       "<polyline fill=\"none\" points=\"218,-189.5 287,-189.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"252.5\" y=\"-174.3\">(None, 2)</text>\n",
       "</g>\n",
       "<!-- 139813355699000 -->\n",
       "<g class=\"node\" id=\"node2\"><title>139813355699000</title>\n",
       "<polygon fill=\"none\" points=\"30.5,-83.5 30.5,-129.5 256.5,-129.5 256.5,-83.5 30.5,-83.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"81.5\" y=\"-102.8\">dense_1: Dense</text>\n",
       "<polyline fill=\"none\" points=\"132.5,-83.5 132.5,-129.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"160\" y=\"-114.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"132.5,-106.5 187.5,-106.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"160\" y=\"-91.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"187.5,-83.5 187.5,-129.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"222\" y=\"-114.3\">(None, 2)</text>\n",
       "<polyline fill=\"none\" points=\"187.5,-106.5 256.5,-106.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"222\" y=\"-91.3\">(None, 2)</text>\n",
       "</g>\n",
       "<!-- 139813355699336&#45;&gt;139813355699000 -->\n",
       "<g class=\"edge\" id=\"edge1\"><title>139813355699336-&gt;139813355699000</title>\n",
       "<path d=\"M143.5,-166.366C143.5,-158.152 143.5,-148.658 143.5,-139.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"147,-139.607 143.5,-129.607 140,-139.607 147,-139.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 139813355698440 -->\n",
       "<g class=\"node\" id=\"node3\"><title>139813355698440</title>\n",
       "<polygon fill=\"none\" points=\"30.5,-0.5 30.5,-46.5 256.5,-46.5 256.5,-0.5 30.5,-0.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"81.5\" y=\"-19.8\">dense_2: Dense</text>\n",
       "<polyline fill=\"none\" points=\"132.5,-0.5 132.5,-46.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"160\" y=\"-31.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"132.5,-23.5 187.5,-23.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"160\" y=\"-8.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"187.5,-0.5 187.5,-46.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"222\" y=\"-31.3\">(None, 2)</text>\n",
       "<polyline fill=\"none\" points=\"187.5,-23.5 256.5,-23.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"222\" y=\"-8.3\">(None, 1)</text>\n",
       "</g>\n",
       "<!-- 139813355699000&#45;&gt;139813355698440 -->\n",
       "<g class=\"edge\" id=\"edge2\"><title>139813355699000-&gt;139813355698440</title>\n",
       "<path d=\"M143.5,-83.3664C143.5,-75.1516 143.5,-65.6579 143.5,-56.7252\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"147,-56.6068 143.5,-46.6068 140,-56.6069 147,-56.6068\" stroke=\"black\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SVG(model_to_dot(model, show_shapes=True).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='mse', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "### Modify the steps per epoch, number of epochs, etc. below as needed. The goal should be 100% accuracy for the XOR data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.2633 - acc: 0.4635\n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.2498 - acc: 0.5000\n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.2461 - acc: 0.5000\n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.1873 - acc: 0.9407\n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.0912 - acc: 1.0000\n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.0482 - acc: 1.0000\n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.0309 - acc: 1.0000\n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.0230 - acc: 1.0000\n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.0187 - acc: 1.0000\n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.0159 - acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f2935978cc0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(data_X, data_y,\n",
    "          steps_per_epoch=1000,\n",
    "          epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "# Run the trained model on the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.07212649],\n",
       "       [0.9314379 ],\n",
       "       [0.92663246],\n",
       "       [0.07268641]], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(data_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "# Question 3:\n",
    "Explain the results of the predict() call above. How well did the trained model do on this problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result of the predict() call above is the model's estimate for the target vector $y$, which is the expected output of the logical XOR function. The error between this vector and the target vector is quantified by the loss function (mean squared error), which returns the difference between the two. It should continue to decrease as more epochs are used, and as this happens, the predicted values should approach the true values of [0, 1, 1, 0].\n",
    "\n",
    "The trained model was able to achieve 100% accuracy, meaning it predicted all four values correctly. Note that despite a 100% accuracy, the loss function is still reporting a non-zero value. This is because the accuracy metric is rounding the output, presumably using a threshold of 0.5 to determine whether to round to 0 or 1. The rounded vector is [0, 1, 1, 0] in this case, which matches the expected output. Thus, the model achieved 100% accuracy. \n",
    "\n",
    "It is worth noting that depending on the run, the model does not always achieve 100% accuracy, even with 10 epochs and 1000 steps per epoch. This suggests some sensitivity to the initial weights, which are most likely random. Sometimes the model converges to 50% or 75% accuracy, meaning it only correctly predicted 2 or 3 out of the four values. Perhaps even more steps and epochs are needed to guarantee convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "# Question 4:\n",
    "Print the weights of both layers of the trained network below. HINT: model.layers gives a list of layers. layer.get_weights() returns layer weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 1\n",
      "Weights:\n",
      "[array([[-4.911073,  4.870712],\n",
      "       [ 4.918023, -4.852263]], dtype=float32), array([-2.8630874, -2.9253035], dtype=float32)]\n",
      "Layer 2\n",
      "Weights:\n",
      "[array([[6.5210857],\n",
      "       [6.6902056]], dtype=float32), array([-3.2473137], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "for i, layer in enumerate(model.layers):\n",
    "    print('Layer %i' % (i+1))\n",
    "    print('Weights:')\n",
    "    print(layer.get_weights())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "For Layer 1, the 2x2 array represents the four weights between the input nodes and the first layer (one column for each node), and the 1x2 array represents the two bias terms, one for each node in the first layer.\n",
    "\n",
    "For Layer 2, the 2x1 array represents the weights between the first and second layer, and the 1x1 array represents the single bias term for the second layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Conceptual Overview\n",
    "\n",
    "The network structure for this portion of the assignment is a multilayer perceptron with a single hidden layer. There are weights between the each layer (input, first, and second), and a bias term for each node in the first and second layers. The purpose of the model is to approximate any function with a continuous mapping from one space to another. In this case, it was used to model the logical XOR function, which is something a single perceptron cannot do. The addition of the hidden layer allows nonlinear behavior to be captured, which adds enormous power to the model. It works by first computing the forward phase, where the output is computed using the current weights and bias, and compared to the expected value. From there, backpropagation is used to update the weights for the next forward phase. This process continues over each row in the input matrix, for the desired number of steps and epochs, where convergence is hopefully achieved.\n",
    "\n",
    "As noted above, most of the time the model converges to 100% accuracy for the XOR data. However, this is not guaranteed. Perhaps adjusting the number of layers, or nodes within each layer, could increase the convergence rate. Despite this, it is still quite impressive how much power is gained by adding just a single hidden layer, and this should not be understated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
