{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Feature scaling through standardization (or Z-score normalization) can be an important preprocessing step for many machine learning algorithms. Standardization involves rescaling the features such that they have the properties of a standard normal distribution with a mean of zero and a standard deviation of one.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Greg DeVore**\n",
    "\n",
    "**ML310**\n",
    "\n",
    "**March 31st, 2018**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1: Know thy data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this problem we'll explore and analyze the pima dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV size: (768, 9)\n",
      "Size of X: (768, 8)\n",
      "Size of y:(768,)\n"
     ]
    }
   ],
   "source": [
    "# Load the Pima Indians diabetes dataset from CSV URL\n",
    "import numpy as np\n",
    "import urllib\n",
    "# URL for the Pima Indians Diabetes dataset (UW Repository)\n",
    "url = \"pima-dataset.csv\"\n",
    "# load the CSV file as a numpy matrix\n",
    "dataset = np.loadtxt(\"pima-dataset.csv\", delimiter=\",\")\n",
    "print('CSV size: ' + str(dataset.shape))\n",
    "# separate the data from the target attributes\n",
    "X = dataset[:,0:8] # returns columns 1 through 8\n",
    "y = dataset[:,8] # returns column 9\n",
    "print('Size of X: ' + str(X.shape))\n",
    "print('Size of y:' + str(y.shape))\n",
    "# read text file for data descriptions\n",
    "textfile = open('pima_desc.txt','r')\n",
    "lines = textfile.readlines()\n",
    "textfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pima dataset columns are described in the file pima_desc.txt The dataset consists of 8 attributes and a binary attribute defining the class label, the presence of diabetes. Data entries are organized in rows such that attributes come first and the class label is last. Answer the following questions with the help of Python:\n",
    "\n",
    "1.\tOutput the Class Distribution: (class value 1 is interpreted as \"tested positive for diabetes\")\n",
    "\n",
    "2.\tWhat is the range (minimum and maximum value) for each of the attributes?\n",
    "\n",
    "3.  Are there any missing values? What are the means and variances of every attribute?\n",
    "\n",
    "4.\tWrite a function normalize that takes an un-normalized vector of attribute values and returns the vector of values     normalized according to the data mean and standard deviation. The normalized value should be:         xnorm = (x - μx)/σx\n",
    "           \n",
    "5. Test your function on attribute 3 of the pima dataset. Report normalized values of the attribute 3 for the first       five entries in the dataset.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Output Class Distribution\n",
    "\n",
    "Here, we'll create a histogram of the response variable and obtain counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY0AAAEKCAYAAADuEgmxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ndfv/1+HPW4XDvBwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd923ba1c90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of patients without diabetes = 500 (response class value 0)\n",
      "Number of patients with diabetes = 268 (response class value 1)\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Output class distribution\n",
    "n,bins,patches = plt.hist(y, bins=10, density=False, facecolor='g', alpha=0.75, align='mid')\n",
    "plt.axis([0, 1, 0, len(y)])\n",
    "plt.xlabel('Response Class Value')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n",
    "class0 = np.sum(y == 0)\n",
    "class1 = np.sum(y == 1)\n",
    "print('Number of patients without diabetes = %i (response class value 0)' % class0)\n",
    "print('Number of patients with diabetes = %i (response class value 1)' % class1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Range of Attributes\n",
    "\n",
    "Here, we'll use the numpy functions 'amax' and 'amin' on the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attribute descriptions:\n",
      "# 1. Number of times pregnant\n",
      "# 2. Plasma glucose concentration a 2 hours in an oral glucose tolerance test\n",
      "# 3. Diastolic blood pressure (mm Hg)\n",
      "# 4. Triceps skin fold thickness (mm)\n",
      "# 5. 2-Hour serum insulin (mu U/ml)\n",
      "# 6. Body mass index (weight in kg/(height in m)^2)\n",
      "# 7. Diabetes pedigree function\n",
      "# 8. Age (years)\n",
      "# 9. Class variable (0 or 1)\n",
      "\n",
      "Attribute min values:\n",
      "[ 0.     0.     0.     0.     0.     0.     0.078 21.   ]\n",
      "\n",
      "Attribute max values:\n",
      "[ 17.   199.   122.    99.   846.    67.1    2.42  81.  ]\n",
      "\n",
      "Attribute ranges:\n",
      "[ 17.    199.    122.     99.    846.     67.1     2.342  60.   ]\n"
     ]
    }
   ],
   "source": [
    "# Max values\n",
    "print('Attribute descriptions:')\n",
    "for line in lines:\n",
    "    print(line.strip())\n",
    "minAttributes = np.amin(X, axis=0)\n",
    "maxAttributes = np.amax(X, axis=0)\n",
    "print('\\nAttribute min values:')\n",
    "print(minAttributes)\n",
    "print('\\nAttribute max values:')\n",
    "print(maxAttributes)\n",
    "print('\\nAttribute ranges:')\n",
    "print(maxAttributes - minAttributes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Missing Values, Mean & Variance\n",
    "\n",
    "Here, we'll look for missing values that are either 'None' or zero (where zero makes no sense)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are no \"None\" type values in the data.\n"
     ]
    }
   ],
   "source": [
    "# Step 1. Check for any values labeled 'None'\n",
    "if np.any(np.equal(X,None)):\n",
    "    print('There are \"None\" type values in the data.')\n",
    "else:\n",
    "    print('There are no \"None\" type values in the data.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values (listed as \"0\") in the following attributes:\n",
      "\n",
      "# 2. Plasma glucose concentration a 2 hours in an oral glucose tolerance test: 5 missing values\n",
      "# 3. Diastolic blood pressure (mm Hg): 35 missing values\n",
      "# 4. Triceps skin fold thickness (mm): 227 missing values\n",
      "# 5. 2-Hour serum insulin (mu U/ml): 374 missing values\n",
      "# 6. Body mass index (weight in kg/(height in m)^2): 11 missing values\n"
     ]
    }
   ],
   "source": [
    "# Step 2. Look for zero values where they don't make sense.\n",
    "# The following attributes have an invalid 'zero' value\n",
    "# 2. Plasma glucose concentration a 2 hours in an oral glucose tolerance test\n",
    "# 3. Diastolic blood pressure (mm Hg)\n",
    "# 4. Triceps skin fold thickness (mm)\n",
    "# 5. 2-Hour serum insulin (mu U/ml)\n",
    "# 6. Body mass index (weight in kg/(height in m)^2)\n",
    "print('Number of missing values (listed as \"0\") in the following attributes:\\n')\n",
    "zeroCols = np.arange(1,6)\n",
    "numZero = np.sum(X[:,zeroCols] == 0, axis=0)\n",
    "for item in zeroCols:\n",
    "    print('%s: %i missing values' % (lines[item].strip(),numZero[item-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should replace the missing values with 'NaN' before computing the mean and variance in order to not skew the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0   5  35 227 374  11   0   0]\n"
     ]
    }
   ],
   "source": [
    "# Remove columns of interest\n",
    "Xslice = X[:,zeroCols]\n",
    "# Replace zeros with NaN\n",
    "Xslice[np.where(Xslice == 0)] = np.nan\n",
    "# Insert back into X\n",
    "X[:,zeroCols] = Xslice\n",
    "# View location and number of NaN's in X, should match previous number of zeros\n",
    "print(sum(np.isnan(X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can calculate the mean and variance of each attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of each attribute:\n",
      "[  3.84505208 121.68676278  72.40518417  29.15341959 155.54822335\n",
      "  32.45746367   0.4718763   33.24088542]\n",
      "Variance of each attribute:\n",
      "[1.13392724e+01 9.31203324e+02 1.53108677e+02 1.09564263e+02\n",
      " 1.40718974e+04 4.78921140e+01 1.09635697e-01 1.38122964e+02]\n"
     ]
    }
   ],
   "source": [
    "# Mean of each attribute (ignoring NaNs)\n",
    "print('Mean of each attribute:')\n",
    "print(np.nanmean(X, axis=0))\n",
    "# Variance of each attribute (ignoring NaNs)\n",
    "print('Variance of each attribute:')\n",
    "print(np.nanvar(X, axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Standardize Attribute\n",
    "\n",
    "Here, we'll create a function that takes a vector and returns a standardized vector (z-score normalization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def standardize(x):\n",
    "    mux = np.nanmean(x)\n",
    "    sigmax = np.sqrt(np.nanvar(x))\n",
    "    xnorm = (x - mux)/sigmax\n",
    "    return xnorm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Test Standardization Function\n",
    "\n",
    "Here, we'll test the standardization function on attribute 3 (Diastolic blood pressure (mm Hg))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standardized mean:\n",
      "-4.87104672195703e-16\n",
      "\n",
      "Standardized standard deviation:\n",
      "0.9999999999999999\n",
      "\n",
      "First five values:\n",
      "[-0.03274557 -0.51764464 -0.67927766 -0.51764464 -2.61887393]\n"
     ]
    }
   ],
   "source": [
    "bp_norm = standardize(X[:,2])\n",
    "# Check that mean and standard deviation are 0 and 1, respectively\n",
    "print('Standardized mean:')\n",
    "print(np.nanmean(bp_norm))\n",
    "print('\\nStandardized standard deviation:')\n",
    "print(np.sqrt(np.nanvar(bp_norm)))\n",
    "# Print first five values\n",
    "print('\\nFirst five values:')\n",
    "print(bp_norm[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2: Decision Trees, Association and Classification Rules  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Briefly outline the major steps of decision tree classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A decision tree (whether for classification or regression) is made up of a series of nodes and branches. At each node, a decision is made based on the value of an attribute within the data set (it could be categorical or numeric). Based on the outcome of the decision, the tree branches left or right, and another node is encountered, with another decision to make. This process continues until a leaf node is reached. A leaf node represents the end of a particular chain of decisions, and contains one of the classes from the response variable. The idea is that when an instance from the data set is passed through the tree, you start at the root node (the top of the tree) and end at a leaf, at which point the most likely class is assigned to the instance based on the value of its attributes.\n",
    "\n",
    "The major steps of decision tree classification are:\n",
    "* Decide which attribute will be used for the root node. For a categorical variable, this is often based the information gained if that attribute is used to split the data (the information gained is directly related to the decrease in entropy from the split). \n",
    "* Repeat this process at subsequent nodes, splitting the remaining instances based on which attributes provide the most information gain (i.e., the greatest reduction in entropy).\n",
    "* Once an entropy of zero has been calculated for a node, stop the process because a leaf node has been reached (no further information is gained because all instances belong to a particular class).\n",
    "* Assign any instances that make it to that particular leaf node the class value of the instances that ended up there during the initial build of the tree.\n",
    "* Recursively apply this process to all nodes until every instance has been classified (i.e., it ends up in a leaf node)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Day | Outlook | Temperature| Humidity  | Wind   | PlayTennis |\n",
    "| ----|---------|------------|-----------|--------|------------|\n",
    "| 1   | Sunny   | Hot        | High      | Weak   | No         |\n",
    "| 2   | Sunny   | Hot        | High      | Strong | No         |\n",
    "| 3   | Overcast| Hot        | High      | Weak   | Yes        |\n",
    "| 4   | Rain    | Mild       | High      | Weak   | Yes        |\n",
    "| 5   | Rain    | Cool       | Normal    | Weak   | Yes        |\n",
    "| 6   | Rain    | Cool       | Normal    | Strong | No         |\n",
    "| 7   | Overcast| Cool       | Normal    | Strong | Yes        |\n",
    "| 8   | Sunny   | Mild       | High      | Weak   | No         |\n",
    "| 9   | Sunny   | Cool       | Normal    | Weak   | Yes        |\n",
    "| 10  | Rain    | Mild       | Normal    | Weak   | Yes        |\n",
    "| 11  | Sunny   | Mild       | Normal    | Strong | Yes        |\n",
    "| 12  | Overcast| Mild       | High      | Strong | Yes        |\n",
    "| 13  | Overcast| Hot        | Normal    | Weak   | Yes        |\n",
    "| 14  | Rain    | Mild       | High      | Strong | No         |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2.\tBuild a decision tree from the given the tennis data set above\n",
    "\n",
    "You should build a tree to predict PlayTennis, based on the other attributes (but, do not use the Day attribute in your tree). Show all of your work, calculations, and decisions as you build the tree. What is the classification accuracy?\n",
    "\n",
    "*Note: Do not use any software package to answer*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll build our tree using entropy and information gain to decide on which attribute each node should be split. First, we need to create functions for calculating the entropy and information gain for a node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to compute the entropy of a single attribute\n",
    "# c is a list of counts for each class\n",
    "def entropySingle(c):\n",
    "    p = np.zeros(len(c))\n",
    "    E = 0\n",
    "    for (i,ci) in enumerate(c):\n",
    "        p[i] = float(ci)/np.sum(c)\n",
    "        if p[i] != 0: # Catch zero probabilities (log is undefined)\n",
    "            E += -p[i] * np.log2(p[i])\n",
    "    return E\n",
    "\n",
    "# Function to compute the entropy for two attributes\n",
    "# cYes is a numpy array of positive class counts for each value of the attribute\n",
    "# cNo is a numpy array of negative class counts for each value of the attribute\n",
    "def entropyDouble(cNo,cYes):\n",
    "    zips = zip(cYes,cNo)\n",
    "    totals = np.array(cYes) + np.array(cNo)\n",
    "    overallTotal = sum(totals)\n",
    "    E = 0\n",
    "    for (i,level) in enumerate(totals):\n",
    "        E += (float(level)/overallTotal)*entropySingle(zips[i])\n",
    "    return E\n",
    "\n",
    "# Function to compute information gain based on splitting by two attributes\n",
    "def infoGain(eS,eD):\n",
    "    # Information gain is difference between entropy of the target (eS) and \n",
    "    # entropy after the target is conditioned on an attribute (eD)\n",
    "    return eS - eD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our functions, we can proceed to build our tree using the instances from the tennis table.\n",
    "\n",
    "First, we need to decide on the root node. To do this, we'll split the response column by each of the different attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Node\n",
      "Target entropy: 0.9402859586706309\n"
     ]
    }
   ],
   "source": [
    "print('Root Node')\n",
    "# Entropy of response variable with no condition\n",
    "Etarget = entropySingle([9,5])\n",
    "print('Target entropy: ' + str(Etarget))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Information gain (Outlook): 0.2467498197744391\n",
      "Information gain (Temperature): 0.029222565658954647\n",
      "Information gain (Humidity): 0.15183550136234136\n",
      "Information gain (Wind): 0.04812703040826927\n"
     ]
    }
   ],
   "source": [
    "# Target entropy conditioned on outlook\n",
    "Eoutlook = entropyDouble([0,2,3],[4,3,2])\n",
    "# Information gain\n",
    "iGoutlook = infoGain(Etarget,Eoutlook)\n",
    "print('Information gain (Outlook): ' + str(iGoutlook))\n",
    "\n",
    "# Target entropy conditioned on temperature\n",
    "Etemp = entropyDouble([1,2,2],[3,2,4])\n",
    "# Information gain\n",
    "iGtemp = infoGain(Etarget,Etemp)\n",
    "print('Information gain (Temperature): ' + str(iGtemp))\n",
    "\n",
    "# Target entropy conditioned on humidity\n",
    "Ehumid = entropyDouble([4,1],[3,6])\n",
    "# Information gain\n",
    "iGhumid = infoGain(Etarget,Ehumid)\n",
    "print('Information gain (Humidity): ' + str(iGhumid))\n",
    "\n",
    "# Target entropy conditioned on wind\n",
    "Ewind = entropyDouble([3,2],[3,6])\n",
    "# Information gain\n",
    "iGwind = infoGain(Etarget,Ewind)\n",
    "print('Information gain (Wind): ' + str(iGwind))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The largest reduction in entropy was from the 'Outlook' attribute, so that will be our root node.\n",
    "\n",
    "Next, we need to repeat the above process after we've split the data based on 'Outlook'. We'll start with Sunny days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Node\n",
      "Target entropy: 0.9709505944546686\n"
     ]
    }
   ],
   "source": [
    "print('Root Node')\n",
    "# Entropy of response variable with Outlook = 'Sunny'\n",
    "Etarget = entropySingle([2,3])\n",
    "print('Target entropy: ' + str(Etarget))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Information gain (Temperature): 0.5709505944546686\n",
      "Information gain (Humidity): 0.9709505944546686\n",
      "Information gain (Wind): 0.01997309402197489\n"
     ]
    }
   ],
   "source": [
    "# Target entropy conditioned on temperature\n",
    "Etemp = entropyDouble([0,2,1],[1,0,1])\n",
    "# Information gain\n",
    "iGtemp = infoGain(Etarget,Etemp)\n",
    "print('Information gain (Temperature): ' + str(iGtemp))\n",
    "\n",
    "# Target entropy conditioned on humidity\n",
    "Ehumid = entropyDouble([3,0],[0,2])\n",
    "# Information gain\n",
    "iGhumid = infoGain(Etarget,Ehumid)\n",
    "print('Information gain (Humidity): ' + str(iGhumid))\n",
    "\n",
    "# Target entropy conditioned on wind\n",
    "Ewind = entropyDouble([1,2],[1,1])\n",
    "# Information gain\n",
    "iGwind = infoGain(Etarget,Ewind)\n",
    "print('Information gain (Wind): ' + str(iGwind))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, splitting on 'Humidity' perfectly splits the remaining instances, so once we split on this we're done.\n",
    "\n",
    "Now, we need to repeat the above process for 'Overcast' days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Node\n",
      "Target entropy: 0.0\n"
     ]
    }
   ],
   "source": [
    "print('Root Node')\n",
    "# Entropy of response variable with Outlook = 'Overcast'\n",
    "Etarget = entropySingle([0,5])\n",
    "print('Target entropy: ' + str(Etarget))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, because the target entropy is zero, all instances already belong to a single class, so we're done here as well.\n",
    "\n",
    "Finally, we need to split the data based for 'Rain' days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Node\n",
      "Target entropy: 0.9709505944546686\n"
     ]
    }
   ],
   "source": [
    "print('Root Node')\n",
    "# Entropy of response variable with Outlook = 'Rain'\n",
    "Etarget = entropySingle([2,3])\n",
    "print('Target entropy: ' + str(Etarget))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Information gain (Temperature): 0.01997309402197489\n",
      "Information gain (Humidity): 0.01997309402197489\n",
      "Information gain (Wind): 0.9709505944546686\n"
     ]
    }
   ],
   "source": [
    "# Target entropy conditioned on temperature\n",
    "Etemp = entropyDouble([1,1],[1,2])\n",
    "# Information gain\n",
    "iGtemp = infoGain(Etarget,Etemp)\n",
    "print('Information gain (Temperature): ' + str(iGtemp))\n",
    "\n",
    "# Target entropy conditioned on humidity\n",
    "Ehumid = entropyDouble([1,1],[1,2])\n",
    "# Information gain\n",
    "iGhumid = infoGain(Etarget,Ehumid)\n",
    "print('Information gain (Humidity): ' + str(iGhumid))\n",
    "\n",
    "# Target entropy conditioned on wind\n",
    "Ewind = entropyDouble([2,0],[0,3])\n",
    "# Information gain\n",
    "iGwind = infoGain(Etarget,Ewind)\n",
    "print('Information gain (Wind): ' + str(iGwind))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, splitting on 'Wind' perfectly splits the remaining instances, so once we split on this we're done.\n",
    "\n",
    "At this point we've completely defined our tree, and the rules are as follows:\n",
    "* If Outlook = 'Sunny' and 'Humidity' = 'High' then 'PlayTennis' = 'No'\n",
    "* If Outlook = 'Sunny' and 'Humidity' = 'Low' then 'PlayTennis' = 'Yes'\n",
    "* If Outlook = 'Overcast' then 'PlayTennis' = 'Yes'\n",
    "* If Outlook = 'Rain' and 'Wind' = 'Strong' then 'PlayTennis' = 'No'\n",
    "* If Outlook = 'Rain' and 'Wind' = 'Weak' then 'PlayTennis' = 'Yes'\n",
    "\n",
    "Note that temperature does not enter in to the decision about whether or not to play. We can turn this tree into a simple Python function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def PlayTennis(Outlook,Temperature,Humidity,Wind):\n",
    "    if Outlook == 'Sunny' and Humidity == 'High':\n",
    "        return 'No'\n",
    "    elif Outlook == 'Sunny' and Humidity == 'Low':\n",
    "        return 'Yes'\n",
    "    elif Outlook == 'Overcast':\n",
    "        return 'Yes'\n",
    "    elif Outlook == 'Rain' and Wind == 'Strong':\n",
    "        return 'No'\n",
    "    elif Outlook == 'Rain' and Wind == 'Weak':\n",
    "        return 'Yes'\n",
    "    else: # Fallthrough case, return majority class\n",
    "        return 'Yes'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test this on our training data. Since this data was used to build the tree, the accuracy should be 100%. To facilitate this, a CSV was created of the tennis data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load tennis data\n",
    "tennisfile = open('tennis.csv')\n",
    "tennis = tennisfile.readlines()\n",
    "tennisfile.close()\n",
    "# Store attributes\n",
    "Outlook = []\n",
    "Temp = []\n",
    "Humidity = []\n",
    "Wind = []\n",
    "Play = []\n",
    "for day in tennis[1:]:\n",
    "    O,T,H,W,P = day.strip().split(',')\n",
    "    Outlook.append(O)\n",
    "    Temp.append(T)\n",
    "    Humidity.append(H)\n",
    "    Wind.append(W)\n",
    "    Play.append(P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predictPlay(Outlook,Temp,Humidity,Wind,Play):\n",
    "    correct = 0    \n",
    "    for (i,Outcome) in enumerate(Play):\n",
    "        Predict = PlayTennis(Outlook[i],Temp[i],Humidity[i],Wind[i])\n",
    "        if Predict == Outcome:\n",
    "            correct += 1\n",
    "    accuracy = float(correct)/len(Play) * 100\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy = 100.000000%\n"
     ]
    }
   ],
   "source": [
    "# Test accuracy of decision tree on training data\n",
    "trainAccuracy = predictPlay(Outlook,Temp,Humidity,Wind,Play)\n",
    "print('Training set accuracy = %f%%' % trainAccuracy)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the training set accuracy is 100% since this data was used to build the tree (i.e., every instance has already been seen)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3.\tIs it possible to produce some set of correct training examples that will get the algorithm to include the attribute Temperature in the learned tree, even though the true target concept is independent of Temperature? \n",
    "If no, explain. If yes, give such a set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, it is possible to select a subset of the training examples that will get the algorithm to include Temperature. There are three levels of temperature, maybe we can try subsets of two of the levels to see if any of them work. There are \"3 choose 2\" or 3 different combinations. First, let's try Hot and Cool:\n",
    "\n",
    "| Day | Outlook | Temperature| Humidity  | Wind   | PlayTennis |\n",
    "| ----|---------|------------|-----------|--------|------------|\n",
    "| 1   | Sunny   | Hot        | High      | Weak   | No         |\n",
    "| 2   | Sunny   | Hot        | High      | Strong | No         |\n",
    "| 3   | Overcast| Hot        | High      | Weak   | Yes        |\n",
    "| 5   | Rain    | Cool       | Normal    | Weak   | Yes        |\n",
    "| 6   | Rain    | Cool       | Normal    | Strong | No         |\n",
    "| 7   | Overcast| Cool       | Normal    | Strong | Yes        |\n",
    "| 9   | Sunny   | Cool       | Normal    | Weak   | Yes        |\n",
    "| 13  | Overcast| Hot        | Normal    | Weak   | Yes        |\n",
    "\n",
    "If we get rid of Hot days where tennis is played and Cool days where it isn't, we get:\n",
    "\n",
    "| Day | Outlook | Temperature| Humidity  | Wind   | PlayTennis |\n",
    "| ----|---------|------------|-----------|--------|------------|\n",
    "| 1   | Sunny   | Hot        | High      | Weak   | No         |\n",
    "| 2   | Sunny   | Hot        | High      | Strong | No         |\n",
    "| 5   | Rain    | Cool       | Normal    | Weak   | Yes        |\n",
    "| 7   | Overcast| Cool       | Normal    | Strong | Yes        |\n",
    "| 9   | Sunny   | Cool       | Normal    | Weak   | Yes        |\n",
    "\n",
    "This example isn't ideal because although Temperature perfectly splits PlayTennis, so does Humidity. \n",
    "\n",
    "Let's try Cool and Mild:\n",
    "\n",
    "| Day | Outlook | Temperature| Humidity  | Wind   | PlayTennis |\n",
    "| ----|---------|------------|-----------|--------|------------|\n",
    "| 4   | Rain    | Mild       | High      | Weak   | Yes        |\n",
    "| 5   | Rain    | Cool       | Normal    | Weak   | Yes        |\n",
    "| 6   | Rain    | Cool       | Normal    | Strong | No         |\n",
    "| 7   | Overcast| Cool       | Normal    | Strong | Yes        |\n",
    "| 8   | Sunny   | Mild       | High      | Weak   | No         |\n",
    "| 9   | Sunny   | Cool       | Normal    | Weak   | Yes        |\n",
    "| 10  | Rain    | Mild       | Normal    | Weak   | Yes        |\n",
    "| 11  | Sunny   | Mild       | Normal    | Strong | Yes        |\n",
    "| 12  | Overcast| Mild       | High      | Strong | Yes        |\n",
    "| 14  | Rain    | Mild       | High      | Strong | No         |\n",
    "\n",
    "If we get rid of Mild days where tennis isn't played and Cool days where it is, we get:\n",
    "\n",
    "| Day | Outlook | Temperature| Humidity  | Wind   | PlayTennis |\n",
    "| ----|---------|------------|-----------|--------|------------|\n",
    "| 4   | Rain    | Mild       | High      | Weak   | Yes        |\n",
    "| 6   | Rain    | Cool       | Normal    | Strong | No         |\n",
    "| 10  | Rain    | Mild       | Normal    | Weak   | Yes        |\n",
    "| 11  | Sunny   | Mild       | Normal    | Strong | Yes        |\n",
    "| 12  | Overcast| Mild       | High      | Strong | Yes        |\n",
    "\n",
    "This data set perfectly splits PlayTennis on Temperature, and not on any other feature. The rule would be\n",
    "* If Temperature == 'Mild' then 'PlayTennis' = 'Yes'\n",
    "* If Temperature == 'Cool' then 'PlayTennis' = 'No'\n",
    "\n",
    "Finally, let's try Hot and Mild:\n",
    "\n",
    "| Day | Outlook | Temperature| Humidity  | Wind   | PlayTennis |\n",
    "| ----|---------|------------|-----------|--------|------------|\n",
    "| 1   | Sunny   | Hot        | High      | Weak   | No         |\n",
    "| 2   | Sunny   | Hot        | High      | Strong | No         |\n",
    "| 3   | Overcast| Hot        | High      | Weak   | Yes        |\n",
    "| 4   | Rain    | Mild       | High      | Weak   | Yes        |\n",
    "| 8   | Sunny   | Mild       | High      | Weak   | No         |\n",
    "| 10  | Rain    | Mild       | Normal    | Weak   | Yes        |\n",
    "| 11  | Sunny   | Mild       | Normal    | Strong | Yes        |\n",
    "| 12  | Overcast| Mild       | High      | Strong | Yes        |\n",
    "| 13  | Overcast| Hot        | Normal    | Weak   | Yes        |\n",
    "| 14  | Rain    | Mild       | High      | Strong | No         |\n",
    "\n",
    "If we get rid of Hot days where tennis is played and Mild days where it isn't, we get:\n",
    "\n",
    "| Day | Outlook | Temperature| Humidity  | Wind   | PlayTennis |\n",
    "| ----|---------|------------|-----------|--------|------------|\n",
    "| 1   | Sunny   | Hot        | High      | Weak   | No         |\n",
    "| 2   | Sunny   | Hot        | High      | Strong | No         |\n",
    "| 4   | Rain    | Mild       | High      | Weak   | Yes        |\n",
    "| 10  | Rain    | Mild       | Normal    | Weak   | Yes        |\n",
    "| 11  | Sunny   | Mild       | Normal    | Strong | Yes        |\n",
    "| 12  | Overcast| Mild       | High      | Strong | Yes        |\n",
    "\n",
    "This data set perfectly splits PlayTennis on Temperature, and not on any other feature. The rule would be\n",
    "* If Temperature == 'Hot' then 'PlayTennis' = 'No'\n",
    "* If Temperature == 'Mild' then 'PlayTennis' = 'Yes'\n",
    "\n",
    "Althought we found two subsets where Temperature would be included in the decision tree, we had to work pretty hard and throw away a lot of data. This illustrates the difficulty in getting an attribute with no real predictive power to be included in the decision tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Build a tree using only examples 1-7. \n",
    "What is the classification accuracy for the training set? what is the accuracy for the test set (examples 8-14)? explain why you think these are the results.  \n",
    "\n",
    "*Note: Do not use any software package to answer*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "For this, we can repeat the effort of 2.2, only with the following training data set:\n",
    "\n",
    "| Day | Outlook | Temperature| Humidity  | Wind   | PlayTennis |\n",
    "| ----|---------|------------|-----------|--------|------------|\n",
    "| 1   | Sunny   | Hot        | High      | Weak   | No         |\n",
    "| 2   | Sunny   | Hot        | High      | Strong | No         |\n",
    "| 3   | Overcast| Hot        | High      | Weak   | Yes        |\n",
    "| 4   | Rain    | Mild       | High      | Weak   | Yes        |\n",
    "| 5   | Rain    | Cool       | Normal    | Weak   | Yes        |\n",
    "| 6   | Rain    | Cool       | Normal    | Strong | No         |\n",
    "| 7   | Overcast| Cool       | Normal    | Strong | Yes        |\n",
    "\n",
    "Along with the following test data set:\n",
    "\n",
    "| Day | Outlook | Temperature| Humidity  | Wind   | PlayTennis |\n",
    "| ----|---------|------------|-----------|--------|------------|\n",
    "| 8   | Sunny   | Mild       | High      | Weak   | No         |\n",
    "| 9   | Sunny   | Cool       | Normal    | Weak   | Yes        |\n",
    "| 10  | Rain    | Mild       | Normal    | Weak   | Yes        |\n",
    "| 11  | Sunny   | Mild       | Normal    | Strong | Yes        |\n",
    "| 12  | Overcast| Mild       | High      | Strong | Yes        |\n",
    "| 13  | Overcast| Hot        | Normal    | Weak   | Yes        |\n",
    "| 14  | Rain    | Mild       | High      | Strong | No         |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Node\n",
      "Target entropy: 0.9852281360342515\n"
     ]
    }
   ],
   "source": [
    "print('Root Node')\n",
    "# Entropy of response variable with no condition\n",
    "Etarget = entropySingle([4,3])\n",
    "print('Target entropy: ' + str(Etarget))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Information gain (Outlook): 0.5916727785823275\n",
      "Information gain (Temperature): 0.19811742113040343\n",
      "Information gain (Humidity): 0.020244207153756077\n",
      "Information gain (Wind): 0.12808527889139443\n"
     ]
    }
   ],
   "source": [
    "# Target entropy conditioned on outlook\n",
    "Eoutlook = entropyDouble([0,1,2],[2,2,0])\n",
    "# Information gain\n",
    "iGoutlook = infoGain(Etarget,Eoutlook)\n",
    "print('Information gain (Outlook): ' + str(iGoutlook))\n",
    "\n",
    "# Target entropy conditioned on temperature\n",
    "Etemp = entropyDouble([1,2,0],[2,1,1])\n",
    "# Information gain\n",
    "iGtemp = infoGain(Etarget,Etemp)\n",
    "print('Information gain (Temperature): ' + str(iGtemp))\n",
    "\n",
    "# Target entropy conditioned on humidity\n",
    "Ehumid = entropyDouble([2,1],[2,2])\n",
    "# Information gain\n",
    "iGhumid = infoGain(Etarget,Ehumid)\n",
    "print('Information gain (Humidity): ' + str(iGhumid))\n",
    "\n",
    "# Target entropy conditioned on wind\n",
    "Ewind = entropyDouble([2,1],[1,3])\n",
    "# Information gain\n",
    "iGwind = infoGain(Etarget,Ewind)\n",
    "print('Information gain (Wind): ' + str(iGwind))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, the biggest information gain is when 'Outlook' is used to split the data. As before, we'll further condition on the three values. First, 'Sunny':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Node\n",
      "Target entropy: 0.0\n"
     ]
    }
   ],
   "source": [
    "print('Root Node')\n",
    "# Entropy of response variable with no condition\n",
    "Etarget = entropySingle([2,0])\n",
    "print('Target entropy: ' + str(Etarget))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the target entropy is zero, all instances belong to the same class and we're done.\n",
    "\n",
    "Next, 'Overcast':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Node\n",
      "Target entropy: 0.0\n"
     ]
    }
   ],
   "source": [
    "print('Root Node')\n",
    "# Entropy of response variable with no condition\n",
    "Etarget = entropySingle([0,2])\n",
    "print('Target entropy: ' + str(Etarget))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, since the target entropy is zero, all instances belong to the same class and we're done.\n",
    "\n",
    "Finally, 'Rain':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Node\n",
      "Target entropy: 0.9182958340544896\n"
     ]
    }
   ],
   "source": [
    "print('Root Node')\n",
    "# Entropy of response variable with no condition\n",
    "Etarget = entropySingle([1,2])\n",
    "print('Target entropy: ' + str(Etarget))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Information gain (Temperature): 0.2516291673878229\n",
      "Information gain (Humidity): 0.2516291673878229\n",
      "Information gain (Wind): 0.9182958340544896\n"
     ]
    }
   ],
   "source": [
    "# Target entropy conditioned on temperature\n",
    "Etemp = entropyDouble([1,0],[1,1])\n",
    "# Information gain\n",
    "iGtemp = infoGain(Etarget,Etemp)\n",
    "print('Information gain (Temperature): ' + str(iGtemp))\n",
    "\n",
    "# Target entropy conditioned on humidity\n",
    "Ehumid = entropyDouble([0,1],[1,1])\n",
    "# Information gain\n",
    "iGhumid = infoGain(Etarget,Ehumid)\n",
    "print('Information gain (Humidity): ' + str(iGhumid))\n",
    "\n",
    "# Target entropy conditioned on wind\n",
    "Ewind = entropyDouble([1,0],[0,2])\n",
    "# Information gain\n",
    "iGwind = infoGain(Etarget,Ewind)\n",
    "print('Information gain (Wind): ' + str(iGwind))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, splitting on 'Wind' perfectly splits the remaining instances, so we're done.\n",
    "\n",
    "At this point we've completely defined our tree, and the rules are as follows:\n",
    "* If Outlook = 'Sunny' then 'PlayTennis' = 'No'\n",
    "* If Outlook = 'Overcast' then 'PlayTennis' = 'Yes'\n",
    "* If Outlook = 'Rain' and 'Wind' = 'Strong' then 'PlayTennis' = 'No'\n",
    "* If Outlook = 'Rain' and 'Wind' = 'Weak' then 'PlayTennis' = 'Yes'\n",
    "\n",
    "As before, we can convert this to a simple Python function. Note that this is a more general set of rules than the first case where we used the entire data set. Now, both Temperature and Humidity are absent from the decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def PlayTennisTrain(Outlook,Temperature,Humidity,Wind):\n",
    "    if Outlook == 'Sunny':\n",
    "        return 'No'\n",
    "    elif Outlook == 'Overcast':\n",
    "        return 'Yes'\n",
    "    elif Outlook == 'Rain' and Wind == 'Strong':\n",
    "        return 'No'\n",
    "    elif Outlook == 'Rain' and Wind == 'Weak':\n",
    "        return 'Yes'\n",
    "    else: # Fallthrough case, return majority class\n",
    "        return 'Yes'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's split the tennis data that was previously loaded into training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "OutlookTrain = Outlook[0:7]\n",
    "OutlookTest = Outlook[7:14]\n",
    "\n",
    "TempTrain = Temp[0:7]\n",
    "TempTest = Temp[7:14]\n",
    "\n",
    "HumidityTrain = Humidity[0:7]\n",
    "HumidityTest = Humidity[7:14]\n",
    "\n",
    "WindTrain = Wind[0:7]\n",
    "WindTest = Wind[7:14]\n",
    "\n",
    "PlayTrain = Play[0:7]\n",
    "PlayTest = Play[7:14]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can compute the accuracy of our new tree on the training and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predictPlaySubset(Outlook,Temp,Humidity,Wind,Play):\n",
    "    correct = 0    \n",
    "    for (i,Outcome) in enumerate(Play):\n",
    "        Predict = PlayTennisTrain(Outlook[i],Temp[i],Humidity[i],Wind[i])\n",
    "        if Predict == Outcome:\n",
    "            correct += 1\n",
    "    accuracy = float(correct)/len(Play) * 100\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy = 100.000000%\n",
      "Test set accuracy = 71.428571%\n"
     ]
    }
   ],
   "source": [
    "# Test accuracy of decision tree on training data\n",
    "trainAccuracy = predictPlaySubset(OutlookTrain,TempTrain,HumidityTrain,WindTrain,PlayTrain)\n",
    "print('Training set accuracy = %f%%' % trainAccuracy) \n",
    "\n",
    "# Test accuracy of decision tree on test data\n",
    "testAccuracy = predictPlaySubset(OutlookTest,TempTest,HumidityTest,WindTest,PlayTest)\n",
    "print('Test set accuracy = %f%%' % testAccuracy) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, the training set accuracy is 100%, which makes sense because that data was used to build the tree. However, the testing set accuracy is only ~71%, which means only 5 out of the 7 test observations was correctly classified. This is because these instances have never been seen by the tree, so there's no guarantee that a correct prediction will be made. Hopefully the training set is large enough that all cases would be covered, but in this case we only used seven observations, and not all cases were covered. As mentioned, Humidity is not used in the tree, even though we know it's an important attribute from when we built the first tree using all of the data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
