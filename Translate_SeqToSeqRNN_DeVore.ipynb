{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "# Sequence-to-Sequence: Language Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "In this assignment you will use a database of pairs of (English,French) sentences to train an RNN model to translate from English to French.\n",
    "\n",
    "The directory ../resource/asnlib/publicdata contains two files, \"small_vocab_en.txt\" and \"small_vocab_fr.txt\". Line \"n\" of the first file corresponds to line \"n\" of the second file.\n",
    "\n",
    "Also see data here: http://www.statmt.org/wmt14/translation-task.html\n",
    "\n",
    "Keras resources: \n",
    "* https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html\n",
    "* https://machinelearningmastery.com/return-sequences-and-return-states-for-lstms-in-keras/\n",
    "* https://stackoverflow.com/questions/38714959/understanding-keras-lstms/50235563#50235563\n",
    "\n",
    "Neural Language Translation Resources:\n",
    "* https://arxiv.org/abs/1703.01619\n",
    "* https://www.tensorflow.org/tutorials/seq2seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "Using TensorFlow backend.\n",
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import string\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython.display import SVG\n",
    "\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from keras.layers import Input, GRU, LSTM, Dense, Masking, Dropout, Embedding, Flatten, Dense\n",
    "from keras.models import Model, load_model\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.regularizers import l1_l2\n",
    "from keras.utils import to_categorical\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "## Configure Tensorflow to be less aggressive about RAM utilization when it starts up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.1  # Start with 10% of the GPU RAM\n",
    "config.gpu_options.allow_growth = True                    # Dynamically grow the memory used on the GPU\n",
    "sess = tf.Session(config=config)\n",
    "set_session(sess)                                         # Set this TensorFlow session as the default session for Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "TRAIN_TEST_SPLIT = 0.7           # % of data in training set\n",
    "\n",
    "NUM_LSTM_NODES = 256             # Num of intermediate LSTM nodes\n",
    "CONTEXT_VECTOR_SIZE = 256        # Size of context vector (num of LSTM nodes in final LSTM layer)\n",
    "\n",
    "EMBEDDING_DIM = 100              # Embedding layer size for input words\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "NUM_EPOCHS = 500\n",
    "\n",
    "NUM_DATA_EXAMPLES = 100000         # limit memory usage while experimenting\n",
    "\n",
    "LR = 0.01\n",
    "DROPOUT = 0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "# Text Preprocessing\n",
    "These are provided so you can focus on the neural net modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A useful string full of characters to remove\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def add_space_around_punctuation(s):\n",
    "    result = ''\n",
    "    for c in s:\n",
    "        if c in string.punctuation and c != \"'\":  # Apostrophes are important\n",
    "            result += ' %s ' % c\n",
    "        else:\n",
    "            result += c\n",
    "    return result\n",
    "\n",
    "def clean_sentence(s):\n",
    "    s = s.strip()\n",
    "    s = s.lower()\n",
    "    s = add_space_around_punctuation(s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "### Functions to get words from sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_words_from_sentence(s, add_start_symbol=False, add_end_symbol=False, reverse=False):\n",
    "    words = list(filter(None, s.split(' ')))\n",
    "    if reverse:\n",
    "        words = words[::-1]\n",
    "    if add_start_symbol:\n",
    "        words = ['<S>'] + words\n",
    "    if add_end_symbol:\n",
    "        words.append('</S>')\n",
    "    return words\n",
    "\n",
    "def get_word_list_from_sentence_string(s, add_start_symbol=False, add_end_symbol=False, reverse=False):\n",
    "    return get_words_from_sentence(clean_sentence(s), add_start_symbol, add_end_symbol, reverse)    \n",
    "    \n",
    "def get_sentences(path, filename, add_start_symbol=False, add_end_symbol=False, reverse=False):\n",
    "    with open(os.path.join(path, filename), 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        return [get_word_list_from_sentence_string(s, add_start_symbol, add_end_symbol, reverse) \n",
    "                for s in lines]\n",
    "\n",
    "def get_word_set(sentences):\n",
    "    words = set()\n",
    "    for s in sentences:\n",
    "        for word in s:\n",
    "            words.add(word)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "## Read the data and build useful data structures, such as a list of sentences and lists of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Store the input sentences (English) in s1\n",
    "# Store the target senteces (French) in s2\n",
    "\n",
    "# Consider reversing the input sentences to improve trianing. - TODO: Try this later\n",
    "# Add start and stop symbols for the decoder.\n",
    "PATH = '../resource/asnlib/publicdata'\n",
    "s1 = get_sentences(PATH, 'small_vocab_en.txt', add_start_symbol=True, add_end_symbol=True, reverse=False)\n",
    "s2 = get_sentences(PATH, 'small_vocab_fr.txt', add_start_symbol=True, add_end_symbol=True, reverse=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check out an example of an English and French sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<S>', 'new', 'jersey', 'is', 'sometimes', 'quiet', 'during', 'autumn', ',', 'and', 'it', 'is', 'snowy', 'in', 'april', '.', '</S>']\n",
      "['<S>', 'new', 'jersey', 'est', 'parfois', 'calme', 'pendant', \"l'\", 'automne', ',', 'et', 'il', 'est', 'neigeux', 'en', 'avril', '.', '</S>']\n"
     ]
    }
   ],
   "source": [
    "print(s1[0])\n",
    "print(s2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137860\n"
     ]
    }
   ],
   "source": [
    "# Look at size of entire data set\n",
    "print(len(s1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go ahead and use the entire data set, so we'll comment out the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Restruct to a subset of the data\n",
    "# s1 = s1[:NUM_DATA_EXAMPLES]\n",
    "# s2 = s2[:NUM_DATA_EXAMPLES]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('<S> new jersey is sometimes quiet during autumn , and it is snowy in april . </S>',\n",
       " \"<S> new jersey est parfois calme pendant l' automne , et il est neigeux en avril . </S>\")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show a sample sentence pair.\n",
    "' '.join(s1[0]), ' '.join(s2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create two lists, w1 and w2, which hold the set of all words that show up in s1 and s2.\n",
    "w1 = get_word_set(s1)\n",
    "w2 = get_word_set(s2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below shows the number of unique English and French words in all sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "204\n",
      "350\n"
     ]
    }
   ],
   "source": [
    "# Print word list lengths\n",
    "print(len(w1))\n",
    "print(len(w2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "### Utilities for mapping words to integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_word_to_index_dict(words):\n",
    "    return {w: i+1 for i,w in enumerate(words)}  # use i+1 to reserve 0 for the mask index\n",
    "def reverse_dict(d):\n",
    "    return {v: k for k,v in d.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "word_to_index1 = get_word_to_index_dict(w1)\n",
    "word_to_index2 = get_word_to_index_dict(w2)\n",
    "index_to_word1 = reverse_dict(word_to_index1)\n",
    "index_to_word2 = reverse_dict(word_to_index2)\n",
    "index_to_word1[0] = '<MASK>'\n",
    "index_to_word2[0] = '<MASK>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def sentence_to_indices(s, word_to_index):\n",
    "    \"\"\"Input s is a sentence string. word_to_index is a dict mapping words to indices.\n",
    "    \n",
    "    This function should convert a sentence to a list of indices, such as [5, 2, 17, 3], and return the list.\"\"\"\n",
    "    return [word_to_index[w] for w in s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def indices_to_sentence(indices, index_to_word):\n",
    "    \"\"\"indices is a list of word indices. word_to_index is a dict mapping indices to words.\n",
    "    \n",
    "    This function should convert the indices list, such as [5, 2, 17, 3], to a list of word strings, and \n",
    "    return the list.\"\"\"\n",
    "    return [index_to_word[i] for i in indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[214, 211, 7, 72, 192]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the functions.\n",
    "x = sentence_to_indices(get_word_list_from_sentence_string('vous aimez raisins.', add_start_symbol=True, reverse=False), word_to_index2)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<S>', 'vous', 'aimez', 'raisins', '.']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Translate back to original sentence\n",
    "indices_to_sentence(x, index_to_word2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(205, 351)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Record the number of words in the input and output data, respectively.\n",
    "num_words_X = len(w1) + 1  # add 1 to reserve 0 for mask\n",
    "num_words_y = len(w2) + 1  # add 1 to reserve 0 for mask\n",
    "num_words_X, num_words_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Convert the input sentences in s1 to a list of sentences each represented as a list of integers.\n",
    "# For example, the output list might look like [[5, 2, 17, 3], [1, 9, 85, 3, 22, 9], ...]\n",
    "# Do the same for the output sentences.\n",
    "inputs_as_indices = [sentence_to_indices(s, word_to_index1) for s in s1]\n",
    "outputs_as_indices = [sentence_to_indices(s, word_to_index2) for s in s2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Now pad the input and output index sequences with a filler (index 0) so that all sequences for each LSTM have the \n",
    "# same length. Use the keras function pad_sequences to do this easily.\n",
    "# Hint: For the inputs, padding should be on the left, like so: [[0, 0, 5, 2, 17, 3], ...]\n",
    "#       For the outputs, padding should be on the right, like so: [[9, 7, 5, 4, 0, 0, 0], ...]\n",
    "inputs = pad_sequences(inputs_as_indices, padding='pre', value=0)\n",
    "outputs = pad_sequences(outputs_as_indices, padding='post', value=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variables below store the longest English and French sentences in the data set. Note that the 'pad_sequences' function forces all sentences to be the maximum length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19, 26)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute the maximum sequence length of the inputs and outputs, just to see how they look.\n",
    "max_seq_len_X = len(inputs[0])\n",
    "max_seq_len_y = len(outputs[0])\n",
    "max_seq_len_X, max_seq_len_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Just for convenience: define some more expressive variable names\n",
    "max_input_seq_len = max_seq_len_X\n",
    "max_output_seq_len = max_seq_len_y\n",
    "num_input_words = num_words_X\n",
    "num_output_words = num_words_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((96501, 19), (41359, 19), (96501, 26), (41359, 26))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create train and test sets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(inputs, outputs, \n",
    "                                                    test_size=1 - TRAIN_TEST_SPLIT,\n",
    "                                                    random_state=42)\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(96501, 26)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the full data set leads to memory errors related to the one-hot encoding procedure. The error is related to initializing the numpy array of zeros for the target data. For this reason, we'll switch the loss function to 'sparse_categorical_crossentropy', which will let us use the response data directly without the need to one-hot encode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# We need to make a one-hot-encoded version of the outputs ourselves for use in the loss function. \n",
    "# The inputs get this for free via use of Embedding layers in Keras.\n",
    "#\n",
    "# Use the keras function to_categorical.\n",
    "# y_train_one_hot = to_categorical(y_train, num_classes=num_words_y)\n",
    "# y_test_one_hot = to_categorical(y_test, num_classes=num_words_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "## Create Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "Now we need to write code to build the SeqToSeq model. **Important**: In Keras we have to use the \"functional API\" in order to access the LSTM internal state that we use as the \"context vector\" or \"encoding\" of a sentence. We also need to store hooks into the model to be able to run the translator on new sentences after training.\n",
    "\n",
    "This code will create variables representing the entire SeqToSeq model (for use in training), as well as the individual encoder segment and decoder segment of the model, for use in inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "We will implement the following architecture for the encoder section of the seq2se1 model:\n",
    "    \n",
    "1. Encoder input (encoder_inputs): Input layer, shape (max_seq_len_X,). For convenience, name the layer: name='encoder_input'\n",
    "2. Masking layer (encoder_masking): doesn't change shape. Ignores leading mask value (\"0\"s) in short sequences.\n",
    "3. Embedding layer (encoder_embedding): output shape (max_seq_len_X, EMBEDDING_DIM)\n",
    "4. LSTM layer: size is NUM_LSTM_NODES. uses dropout at rate given by DROPOUT.\n",
    "\n",
    "Hint: Be sure to set the \"return_sequences\" and \"return_state\" parameters appropriately in the LSTM for the encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Build RNN model.\n",
    "encoder_inputs = Input(shape=(max_input_seq_len,), name='encoder_input')\n",
    "encoder_masking = Masking(mask_value=0)(encoder_inputs)\n",
    "encoder_embedding = Embedding(num_words_X+1, EMBEDDING_DIM)(encoder_masking)\n",
    "encoder_outputs, state_h, state_c = LSTM(NUM_LSTM_NODES, dropout=DROPOUT, return_sequences=False, return_state=True, name='encoder_lstm_1')(encoder_embedding) \n",
    "# Discard `encoder_outputs` and only keep the states. We don't use the outputs in the encoder.\n",
    "# Recall that the LSTM has two states we have to keep track of: c and h.\n",
    "encoder_states = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "# The decoder should have the following architecture:\n",
    "    \n",
    "1. Decoder input (decoder_input): shape (None,)\n",
    "2. Masking layer (decoder_masking), as above.\n",
    "3. Embedding layer (decoder_embedding): output shape (max_seq_len_y, EMBEDDING_DIM)\n",
    "4. LSTM layer (decoder_lstm), as above. However, keep a function around to easy recreate the LSTM layer later on, during generation.\n",
    "6. Dense layer with softmax activation (decoder_output): output shape (num_output_words,)\n",
    "\n",
    "Hint: Be sure to set the \"return_sequences\" and \"return_state\" parameters appropriately in the LSTM for the encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Decoder section\n",
    "# Set up the decoder, using encoder_states as initial state.\n",
    "decoder_inputs = Input(shape=(None,), name='decoder_input')\n",
    "decoder_inputs_masking = Masking(mask_value=0)(decoder_inputs)\n",
    "decoder_inputs_embedded = Embedding(num_words_y+1, EMBEDDING_DIM)(decoder_inputs_masking)\n",
    "decoder_lstm = LSTM(NUM_LSTM_NODES, dropout=DROPOUT, return_sequences=True, return_state=True) # Just define an LSTM here, but don't pass in the previous layer variable yet.\n",
    "\n",
    "z, _, _ = decoder_lstm(decoder_inputs_embedded, initial_state=encoder_states) # Pass in the context vector using the \"initial_state\" param\n",
    "\n",
    "decoder_dense = Dense(num_words_y, activation='softmax') # Like LSTM above: define function for later use\n",
    "decoder_outputs = decoder_dense(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "### Put it all together into one model, and visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_input (InputLayer)      (None, 19)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "decoder_input (InputLayer)      (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "masking_1 (Masking)             (None, 19)           0           encoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "masking_2 (Masking)             (None, None)         0           decoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 19, 100)      20600       masking_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, None, 100)    35200       masking_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "encoder_lstm_1 (LSTM)           [(None, 256), (None, 365568      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, None, 256),  365568      embedding_2[0][0]                \n",
      "                                                                 encoder_lstm_1[0][1]             \n",
      "                                                                 encoder_lstm_1[0][2]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, None, 351)    90207       lstm_1[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 877,143\n",
      "Trainable params: 877,143\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define the complete seq2seq model.\n",
    "# This will take encoder_input_data & decoder_input_data as inputs and learn to output the decoder_target_data.\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"470pt\" viewBox=\"0.00 0.00 803.50 470.00\" width=\"804pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 466)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"white\" points=\"-4,4 -4,-466 799.5,-466 799.5,4 -4,4\" stroke=\"none\"/>\n",
       "<!-- 139727419946096 -->\n",
       "<g class=\"node\" id=\"node1\"><title>139727419946096</title>\n",
       "<polygon fill=\"none\" points=\"75,-415.5 75,-461.5 367,-461.5 367,-415.5 75,-415.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"155.5\" y=\"-434.8\">encoder_input: InputLayer</text>\n",
       "<polyline fill=\"none\" points=\"236,-415.5 236,-461.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"263.5\" y=\"-446.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"236,-438.5 291,-438.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"263.5\" y=\"-423.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"291,-415.5 291,-461.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"329\" y=\"-446.3\">(None, 19)</text>\n",
       "<polyline fill=\"none\" points=\"291,-438.5 367,-438.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"329\" y=\"-423.3\">(None, 19)</text>\n",
       "</g>\n",
       "<!-- 139727419945088 -->\n",
       "<g class=\"node\" id=\"node3\"><title>139727419945088</title>\n",
       "<polygon fill=\"none\" points=\"89.5,-332.5 89.5,-378.5 352.5,-378.5 352.5,-332.5 89.5,-332.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"155.5\" y=\"-351.8\">masking_1: Masking</text>\n",
       "<polyline fill=\"none\" points=\"221.5,-332.5 221.5,-378.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"249\" y=\"-363.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"221.5,-355.5 276.5,-355.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"249\" y=\"-340.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"276.5,-332.5 276.5,-378.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"314.5\" y=\"-363.3\">(None, 19)</text>\n",
       "<polyline fill=\"none\" points=\"276.5,-355.5 352.5,-355.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"314.5\" y=\"-340.3\">(None, 19)</text>\n",
       "</g>\n",
       "<!-- 139727419946096&#45;&gt;139727419945088 -->\n",
       "<g class=\"edge\" id=\"edge1\"><title>139727419946096-&gt;139727419945088</title>\n",
       "<path d=\"M221,-415.366C221,-407.152 221,-397.658 221,-388.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"224.5,-388.607 221,-378.607 217.5,-388.607 224.5,-388.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 139727418716568 -->\n",
       "<g class=\"node\" id=\"node2\"><title>139727418716568</title>\n",
       "<polygon fill=\"none\" points=\"474,-332.5 474,-378.5 782,-378.5 782,-332.5 474,-332.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"554.5\" y=\"-351.8\">decoder_input: InputLayer</text>\n",
       "<polyline fill=\"none\" points=\"635,-332.5 635,-378.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"662.5\" y=\"-363.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"635,-355.5 690,-355.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"662.5\" y=\"-340.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"690,-332.5 690,-378.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"736\" y=\"-363.3\">(None, None)</text>\n",
       "<polyline fill=\"none\" points=\"690,-355.5 782,-355.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"736\" y=\"-340.3\">(None, None)</text>\n",
       "</g>\n",
       "<!-- 139727418716400 -->\n",
       "<g class=\"node\" id=\"node4\"><title>139727418716400</title>\n",
       "<polygon fill=\"none\" points=\"488.5,-249.5 488.5,-295.5 767.5,-295.5 767.5,-249.5 488.5,-249.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"554.5\" y=\"-268.8\">masking_2: Masking</text>\n",
       "<polyline fill=\"none\" points=\"620.5,-249.5 620.5,-295.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"648\" y=\"-280.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"620.5,-272.5 675.5,-272.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"648\" y=\"-257.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"675.5,-249.5 675.5,-295.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"721.5\" y=\"-280.3\">(None, None)</text>\n",
       "<polyline fill=\"none\" points=\"675.5,-272.5 767.5,-272.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"721.5\" y=\"-257.3\">(None, None)</text>\n",
       "</g>\n",
       "<!-- 139727418716568&#45;&gt;139727418716400 -->\n",
       "<g class=\"edge\" id=\"edge2\"><title>139727418716568-&gt;139727418716400</title>\n",
       "<path d=\"M628,-332.366C628,-324.152 628,-314.658 628,-305.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"631.5,-305.607 628,-295.607 624.5,-305.607 631.5,-305.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 139727419946376 -->\n",
       "<g class=\"node\" id=\"node5\"><title>139727419946376</title>\n",
       "<polygon fill=\"none\" points=\"61,-249.5 61,-295.5 381,-295.5 381,-249.5 61,-249.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"141.5\" y=\"-268.8\">embedding_1: Embedding</text>\n",
       "<polyline fill=\"none\" points=\"222,-249.5 222,-295.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"249.5\" y=\"-280.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"222,-272.5 277,-272.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"249.5\" y=\"-257.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"277,-249.5 277,-295.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"329\" y=\"-280.3\">(None, 19)</text>\n",
       "<polyline fill=\"none\" points=\"277,-272.5 381,-272.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"329\" y=\"-257.3\">(None, 19, 100)</text>\n",
       "</g>\n",
       "<!-- 139727419945088&#45;&gt;139727419946376 -->\n",
       "<g class=\"edge\" id=\"edge3\"><title>139727419945088-&gt;139727419946376</title>\n",
       "<path d=\"M221,-332.366C221,-324.152 221,-314.658 221,-305.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"224.5,-305.607 221,-295.607 217.5,-305.607 224.5,-305.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 139727418717856 -->\n",
       "<g class=\"node\" id=\"node6\"><title>139727418717856</title>\n",
       "<polygon fill=\"none\" points=\"460.5,-166.5 460.5,-212.5 795.5,-212.5 795.5,-166.5 460.5,-166.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"541\" y=\"-185.8\">embedding_2: Embedding</text>\n",
       "<polyline fill=\"none\" points=\"621.5,-166.5 621.5,-212.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"649\" y=\"-197.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"621.5,-189.5 676.5,-189.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"649\" y=\"-174.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"676.5,-166.5 676.5,-212.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"736\" y=\"-197.3\">(None, None)</text>\n",
       "<polyline fill=\"none\" points=\"676.5,-189.5 795.5,-189.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"736\" y=\"-174.3\">(None, None, 100)</text>\n",
       "</g>\n",
       "<!-- 139727418716400&#45;&gt;139727418717856 -->\n",
       "<g class=\"edge\" id=\"edge4\"><title>139727418716400-&gt;139727418717856</title>\n",
       "<path d=\"M628,-249.366C628,-241.152 628,-231.658 628,-222.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"631.5,-222.607 628,-212.607 624.5,-222.607 631.5,-222.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 139727419946544 -->\n",
       "<g class=\"node\" id=\"node7\"><title>139727419946544</title>\n",
       "<polygon fill=\"none\" points=\"0,-166.5 0,-212.5 442,-212.5 442,-166.5 0,-166.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"74\" y=\"-185.8\">encoder_lstm_1: LSTM</text>\n",
       "<polyline fill=\"none\" points=\"148,-166.5 148,-212.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"175.5\" y=\"-197.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"148,-189.5 203,-189.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"175.5\" y=\"-174.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"203,-166.5 203,-212.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"322.5\" y=\"-197.3\">(None, 19, 100)</text>\n",
       "<polyline fill=\"none\" points=\"203,-189.5 442,-189.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"322.5\" y=\"-174.3\">[(None, 256), (None, 256), (None, 256)]</text>\n",
       "</g>\n",
       "<!-- 139727419946376&#45;&gt;139727419946544 -->\n",
       "<g class=\"edge\" id=\"edge5\"><title>139727419946376-&gt;139727419946544</title>\n",
       "<path d=\"M221,-249.366C221,-241.152 221,-231.658 221,-222.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"224.5,-222.607 221,-212.607 217.5,-222.607 224.5,-222.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 139727418716680 -->\n",
       "<g class=\"node\" id=\"node8\"><title>139727418716680</title>\n",
       "<polygon fill=\"none\" points=\"210,-83.5 210,-129.5 638,-129.5 638,-83.5 210,-83.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"259\" y=\"-102.8\">lstm_1: LSTM</text>\n",
       "<polyline fill=\"none\" points=\"308,-83.5 308,-129.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"335.5\" y=\"-114.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"308,-106.5 363,-106.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"335.5\" y=\"-91.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"363,-83.5 363,-129.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"500.5\" y=\"-114.3\">[(None, None, 100), (None, 256), (None, 256)]</text>\n",
       "<polyline fill=\"none\" points=\"363,-106.5 638,-106.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"500.5\" y=\"-91.3\">[(None, None, 256), (None, 256), (None, 256)]</text>\n",
       "</g>\n",
       "<!-- 139727418717856&#45;&gt;139727418716680 -->\n",
       "<g class=\"edge\" id=\"edge6\"><title>139727418717856-&gt;139727418716680</title>\n",
       "<path d=\"M572.526,-166.473C546.668,-156.206 515.733,-143.924 488.806,-133.232\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"490.049,-129.96 479.463,-129.522 487.466,-136.466 490.049,-129.96\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 139727419946544&#45;&gt;139727418716680 -->\n",
       "<g class=\"edge\" id=\"edge7\"><title>139727419946544-&gt;139727418716680</title>\n",
       "<path d=\"M276.203,-166.473C301.933,-156.206 332.716,-143.924 359.512,-133.232\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"360.818,-136.479 368.808,-129.522 358.223,-129.978 360.818,-136.479\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 139727418825080 -->\n",
       "<g class=\"node\" id=\"node9\"><title>139727418825080</title>\n",
       "<polygon fill=\"none\" points=\"286,-0.5 286,-46.5 562,-46.5 562,-0.5 286,-0.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"337\" y=\"-19.8\">dense_1: Dense</text>\n",
       "<polyline fill=\"none\" points=\"388,-0.5 388,-46.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"415.5\" y=\"-31.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"388,-23.5 443,-23.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"415.5\" y=\"-8.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"443,-0.5 443,-46.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"502.5\" y=\"-31.3\">(None, None, 256)</text>\n",
       "<polyline fill=\"none\" points=\"443,-23.5 562,-23.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"502.5\" y=\"-8.3\">(None, None, 351)</text>\n",
       "</g>\n",
       "<!-- 139727418716680&#45;&gt;139727418825080 -->\n",
       "<g class=\"edge\" id=\"edge9\"><title>139727418716680-&gt;139727418825080</title>\n",
       "<path d=\"M424,-83.3664C424,-75.1516 424,-65.6579 424,-56.7252\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"427.5,-56.6068 424,-46.6068 420.5,-56.6069 427.5,-56.6068\" stroke=\"black\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SVG(model_to_dot(model, show_shapes=True).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    },
    "scrolled": false
   },
   "source": [
    "## Prepare train/test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "encoder_input_data = X_train\n",
    "decoder_input_data = y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned above, we're not using the one-hot encoded based process to avoid memory errors. The target data is formed in nearly the same manner, however the arrays are smaller."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "decoder_target_data = np.zeros(y_train.shape)\n",
    "decoder_target_data[:,:-1] = y_train[:,1:]\n",
    "decoder_target_data_test = np.zeros(y_test.shape)\n",
    "decoder_target_data_test[:,:-1] = y_test[:,1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the decoder target data is still ahead of the training data by a single timestep, as intended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[214  95 160 322 216 259 129 110 305 166 322 227 121 234  95 236 192 176\n",
      "   0   0   0   0   0   0   0   0]\n"
     ]
    }
   ],
   "source": [
    "print(y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 95. 160. 322. 216. 259. 129. 110. 305. 166. 322. 227. 121. 234.  95.\n",
      " 236. 192. 176.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n"
     ]
    }
   ],
   "source": [
    "print(decoder_target_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the model is expecting the decoder target data to have three dimensions, we have to perform the reshape procedure below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "decoder_target_data = np.reshape(decoder_target_data,(decoder_target_data.shape[0],decoder_target_data.shape[1],1))\n",
    "decoder_target_data_test = np.reshape(decoder_target_data_test,(decoder_target_data_test.shape[0],decoder_target_data_test.shape[1],1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# decoder_target_data will be ahead by one timestep\n",
    "# and will not include the start token.\n",
    "# decoder_target_data = np.zeros(y_train_one_hot.shape)\n",
    "# decoder_target_data[:,:-1] = y_train_one_hot[:,1:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# decoder_target_data_test = np.zeros(y_test_one_hot.shape)\n",
    "# decoder_target_data_test[:,:-1] = y_test_one_hot[:,1:,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As noted above, we're changing the loss function to 'sparse_categorical_crossentropy'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "optimizer = Adam(lr=.001)\n",
    "model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr_callback = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=4, verbose=1, mode='auto', \n",
    "                                cooldown=0, min_lr=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "early_stopping_callback = EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=1, mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 96501 samples, validate on 41359 samples\n",
      "Epoch 1/500\n",
      "96501/96501 [==============================] - 409s 4ms/step - loss: 0.8326 - val_loss: 0.4315\n",
      "Epoch 2/500\n",
      "96501/96501 [==============================] - 413s 4ms/step - loss: 0.3145 - val_loss: 0.1654\n",
      "Epoch 3/500\n",
      "96501/96501 [==============================] - 419s 4ms/step - loss: 0.0951 - val_loss: 0.0444\n",
      "Epoch 4/500\n",
      "96501/96501 [==============================] - 424s 4ms/step - loss: 0.0384 - val_loss: 0.0271\n",
      "Epoch 5/500\n",
      "96501/96501 [==============================] - 422s 4ms/step - loss: 0.0260 - val_loss: 0.0215\n",
      "Epoch 6/500\n",
      "96501/96501 [==============================] - 429s 4ms/step - loss: 0.0198 - val_loss: 0.0169\n",
      "Epoch 7/500\n",
      "96501/96501 [==============================] - 433s 4ms/step - loss: 0.0160 - val_loss: 0.0145\n",
      "Epoch 8/500\n",
      "96501/96501 [==============================] - 856s 9ms/step - loss: 0.0134 - val_loss: 0.0129\n",
      "Epoch 9/500\n",
      "96501/96501 [==============================] - 785s 8ms/step - loss: 0.0117 - val_loss: 0.0112\n",
      "Epoch 10/500\n",
      "96501/96501 [==============================] - 883s 9ms/step - loss: 0.0103 - val_loss: 0.0111\n",
      "Epoch 11/500\n",
      "96501/96501 [==============================] - 670s 7ms/step - loss: 0.0094 - val_loss: 0.0102\n",
      "Epoch 12/500\n",
      "96501/96501 [==============================] - 716s 7ms/step - loss: 0.0083 - val_loss: 0.0100\n",
      "Epoch 13/500\n",
      "96501/96501 [==============================] - 814s 8ms/step - loss: 0.0079 - val_loss: 0.0093\n",
      "Epoch 14/500\n",
      "96501/96501 [==============================] - 815s 8ms/step - loss: 0.0070 - val_loss: 0.0089\n",
      "Epoch 15/500\n",
      "96501/96501 [==============================] - 814s 8ms/step - loss: 0.0066 - val_loss: 0.0092\n",
      "Epoch 16/500\n",
      "96501/96501 [==============================] - 804s 8ms/step - loss: 0.0059 - val_loss: 0.0082\n",
      "Epoch 17/500\n",
      "96501/96501 [==============================] - 806s 8ms/step - loss: 0.0059 - val_loss: 0.0082\n",
      "Epoch 18/500\n",
      "96501/96501 [==============================] - 1024s 11ms/step - loss: 0.0054 - val_loss: 0.0122\n",
      "Epoch 19/500\n",
      "96501/96501 [==============================] - 994s 10ms/step - loss: 0.0051 - val_loss: 0.0079\n",
      "Epoch 20/500\n",
      "96501/96501 [==============================] - 596s 6ms/step - loss: 0.0050 - val_loss: 0.0077\n",
      "Epoch 21/500\n",
      "96501/96501 [==============================] - 418s 4ms/step - loss: 0.0045 - val_loss: 0.0076\n",
      "Epoch 22/500\n",
      "96501/96501 [==============================] - 416s 4ms/step - loss: 0.0042 - val_loss: 0.0076\n",
      "Epoch 23/500\n",
      "96501/96501 [==============================] - 480s 5ms/step - loss: 0.0042 - val_loss: 0.0077\n",
      "Epoch 24/500\n",
      "96501/96501 [==============================] - 527s 5ms/step - loss: 0.0041 - val_loss: 0.0072\n",
      "Epoch 25/500\n",
      "96501/96501 [==============================] - 468s 5ms/step - loss: 0.0039 - val_loss: 0.0076\n",
      "Epoch 26/500\n",
      "96501/96501 [==============================] - 412s 4ms/step - loss: 0.0038 - val_loss: 0.0074\n",
      "Epoch 27/500\n",
      "96501/96501 [==============================] - 420s 4ms/step - loss: 0.0034 - val_loss: 0.0075\n",
      "Epoch 28/500\n",
      "96501/96501 [==============================] - 420s 4ms/step - loss: 0.0033 - val_loss: 0.0074\n",
      "Epoch 29/500\n",
      "96501/96501 [==============================] - 450s 5ms/step - loss: 0.0032 - val_loss: 0.0070\n",
      "Epoch 30/500\n",
      "96501/96501 [==============================] - 410s 4ms/step - loss: 0.0033 - val_loss: 0.0073\n",
      "Epoch 31/500\n",
      "96501/96501 [==============================] - 409s 4ms/step - loss: 0.0031 - val_loss: 0.0072\n",
      "Epoch 32/500\n",
      "96501/96501 [==============================] - 411s 4ms/step - loss: 0.0029 - val_loss: 0.0074\n",
      "Epoch 33/500\n",
      "96501/96501 [==============================] - 471s 5ms/step - loss: 0.0029 - val_loss: 0.0076\n",
      "Epoch 34/500\n",
      "96501/96501 [==============================] - 411s 4ms/step - loss: 0.0028 - val_loss: 0.0075\n",
      "\n",
      "Epoch 00034: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 35/500\n",
      "96501/96501 [==============================] - 476s 5ms/step - loss: 0.0017 - val_loss: 0.0065\n",
      "Epoch 36/500\n",
      "96501/96501 [==============================] - 410s 4ms/step - loss: 0.0013 - val_loss: 0.0064\n",
      "Epoch 37/500\n",
      "96501/96501 [==============================] - 408s 4ms/step - loss: 0.0013 - val_loss: 0.0066\n",
      "Epoch 38/500\n",
      "96501/96501 [==============================] - 413s 4ms/step - loss: 0.0012 - val_loss: 0.0065\n",
      "Epoch 39/500\n",
      "96501/96501 [==============================] - 411s 4ms/step - loss: 0.0012 - val_loss: 0.0067\n",
      "Epoch 40/500\n",
      "96501/96501 [==============================] - 411s 4ms/step - loss: 0.0011 - val_loss: 0.0068\n",
      "Epoch 41/500\n",
      "96501/96501 [==============================] - 411s 4ms/step - loss: 0.0011 - val_loss: 0.0066\n",
      "\n",
      "Epoch 00041: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 42/500\n",
      "96501/96501 [==============================] - 413s 4ms/step - loss: 7.7612e-04 - val_loss: 0.0064\n",
      "Epoch 43/500\n",
      "96501/96501 [==============================] - 413s 4ms/step - loss: 6.2999e-04 - val_loss: 0.0065\n",
      "Epoch 44/500\n",
      "96501/96501 [==============================] - 413s 4ms/step - loss: 6.0176e-04 - val_loss: 0.0065\n",
      "Epoch 45/500\n",
      "96501/96501 [==============================] - 409s 4ms/step - loss: 5.8896e-04 - val_loss: 0.0066\n",
      "\n",
      "Epoch 00045: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 46/500\n",
      "96501/96501 [==============================] - 408s 4ms/step - loss: 4.8809e-04 - val_loss: 0.0066\n",
      "Epoch 00046: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f15a9cfcf98>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run training\n",
    "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "          batch_size=BATCH_SIZE,\n",
    "          epochs=NUM_EPOCHS,\n",
    "          validation_data=([X_test, y_test], decoder_target_data_test),\n",
    "          callbacks=[lr_callback, early_stopping_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "We have trained a model, but how do we use it to actually translate sentences? We have to do more work ourselves here than with a non-recurrent neural net, so we'll write a function to help out. Here are the steps:\n",
    "\n",
    "1. **Encode**:\n",
    "    1. Run the entire input sentence through the encoder part of the model.\n",
    "    1. Write down the \"context vector\" -- this is the state of the last LSTM encoder layer.<br><br>\n",
    "\n",
    "2. **Decode in a loop**:\n",
    "    1. Seed the decoder LSTM with the context vector.\n",
    "    1. Run a *single step* of the decoder with the input \"`<S>`\" (the start symbol).\n",
    "    1. Store the output. This is a word of the translation!\n",
    "    1. Return to step 2B, but feed in the word from step 2C as the new input. Repeat until the decoder returns \"`</S>`\" (the end symbol)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "encoder_input (InputLayer)   (None, 19)                0         \n",
      "_________________________________________________________________\n",
      "masking_1 (Masking)          (None, 19)                0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 19, 100)           20600     \n",
      "_________________________________________________________________\n",
      "encoder_lstm_1 (LSTM)        [(None, 256), (None, 256) 365568    \n",
      "=================================================================\n",
      "Total params: 386,168\n",
      "Trainable params: 386,168\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define a version of our model for use in sampling (as opposed to training).\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "encoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"304pt\" viewBox=\"0.00 0.00 450.00 304.00\" width=\"450pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 300)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"white\" points=\"-4,4 -4,-300 446,-300 446,4 -4,4\" stroke=\"none\"/>\n",
       "<!-- 139727419946096 -->\n",
       "<g class=\"node\" id=\"node1\"><title>139727419946096</title>\n",
       "<polygon fill=\"none\" points=\"75,-249.5 75,-295.5 367,-295.5 367,-249.5 75,-249.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"155.5\" y=\"-268.8\">encoder_input: InputLayer</text>\n",
       "<polyline fill=\"none\" points=\"236,-249.5 236,-295.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"263.5\" y=\"-280.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"236,-272.5 291,-272.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"263.5\" y=\"-257.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"291,-249.5 291,-295.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"329\" y=\"-280.3\">(None, 19)</text>\n",
       "<polyline fill=\"none\" points=\"291,-272.5 367,-272.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"329\" y=\"-257.3\">(None, 19)</text>\n",
       "</g>\n",
       "<!-- 139727419945088 -->\n",
       "<g class=\"node\" id=\"node2\"><title>139727419945088</title>\n",
       "<polygon fill=\"none\" points=\"89.5,-166.5 89.5,-212.5 352.5,-212.5 352.5,-166.5 89.5,-166.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"155.5\" y=\"-185.8\">masking_1: Masking</text>\n",
       "<polyline fill=\"none\" points=\"221.5,-166.5 221.5,-212.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"249\" y=\"-197.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"221.5,-189.5 276.5,-189.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"249\" y=\"-174.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"276.5,-166.5 276.5,-212.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"314.5\" y=\"-197.3\">(None, 19)</text>\n",
       "<polyline fill=\"none\" points=\"276.5,-189.5 352.5,-189.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"314.5\" y=\"-174.3\">(None, 19)</text>\n",
       "</g>\n",
       "<!-- 139727419946096&#45;&gt;139727419945088 -->\n",
       "<g class=\"edge\" id=\"edge1\"><title>139727419946096-&gt;139727419945088</title>\n",
       "<path d=\"M221,-249.366C221,-241.152 221,-231.658 221,-222.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"224.5,-222.607 221,-212.607 217.5,-222.607 224.5,-222.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 139727419946376 -->\n",
       "<g class=\"node\" id=\"node3\"><title>139727419946376</title>\n",
       "<polygon fill=\"none\" points=\"61,-83.5 61,-129.5 381,-129.5 381,-83.5 61,-83.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"141.5\" y=\"-102.8\">embedding_1: Embedding</text>\n",
       "<polyline fill=\"none\" points=\"222,-83.5 222,-129.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"249.5\" y=\"-114.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"222,-106.5 277,-106.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"249.5\" y=\"-91.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"277,-83.5 277,-129.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"329\" y=\"-114.3\">(None, 19)</text>\n",
       "<polyline fill=\"none\" points=\"277,-106.5 381,-106.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"329\" y=\"-91.3\">(None, 19, 100)</text>\n",
       "</g>\n",
       "<!-- 139727419945088&#45;&gt;139727419946376 -->\n",
       "<g class=\"edge\" id=\"edge2\"><title>139727419945088-&gt;139727419946376</title>\n",
       "<path d=\"M221,-166.366C221,-158.152 221,-148.658 221,-139.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"224.5,-139.607 221,-129.607 217.5,-139.607 224.5,-139.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 139727419946544 -->\n",
       "<g class=\"node\" id=\"node4\"><title>139727419946544</title>\n",
       "<polygon fill=\"none\" points=\"-2.84217e-14,-0.5 -2.84217e-14,-46.5 442,-46.5 442,-0.5 -2.84217e-14,-0.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"74\" y=\"-19.8\">encoder_lstm_1: LSTM</text>\n",
       "<polyline fill=\"none\" points=\"148,-0.5 148,-46.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"175.5\" y=\"-31.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"148,-23.5 203,-23.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"175.5\" y=\"-8.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"203,-0.5 203,-46.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"322.5\" y=\"-31.3\">(None, 19, 100)</text>\n",
       "<polyline fill=\"none\" points=\"203,-23.5 442,-23.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"322.5\" y=\"-8.3\">[(None, 256), (None, 256), (None, 256)]</text>\n",
       "</g>\n",
       "<!-- 139727419946376&#45;&gt;139727419946544 -->\n",
       "<g class=\"edge\" id=\"edge3\"><title>139727419946376-&gt;139727419946544</title>\n",
       "<path d=\"M221,-83.3664C221,-75.1516 221,-65.6579 221,-56.7252\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"224.5,-56.6068 221,-46.6068 217.5,-56.6069 224.5,-56.6068\" stroke=\"black\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SVG(model_to_dot(encoder_model, show_shapes=True).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "decoder_input (InputLayer)      (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "masking_2 (Masking)             (None, None)         0           decoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, None, 100)    35200       masking_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            (None, 256)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 256)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, None, 256),  365568      embedding_2[0][0]                \n",
      "                                                                 input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, None, 351)    90207       lstm_1[1][0]                     \n",
      "==================================================================================================\n",
      "Total params: 490,975\n",
      "Trainable params: 490,975\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Build the decoder.\n",
    "decoder_state_input_h = Input(shape=(NUM_LSTM_NODES,))\n",
    "decoder_state_input_c = Input(shape=(NUM_LSTM_NODES,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "    decoder_inputs_embedded, initial_state=decoder_states_inputs)\n",
    "\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model([decoder_inputs] + decoder_states_inputs,\n",
    "                      [decoder_outputs] + decoder_states)\n",
    "decoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"387pt\" viewBox=\"0.00 0.00 905.00 387.00\" width=\"905pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 383)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"white\" points=\"-4,4 -4,-383 901,-383 901,4 -4,4\" stroke=\"none\"/>\n",
       "<!-- 139727418716568 -->\n",
       "<g class=\"node\" id=\"node1\"><title>139727418716568</title>\n",
       "<polygon fill=\"none\" points=\"13.5,-332.5 13.5,-378.5 321.5,-378.5 321.5,-332.5 13.5,-332.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"94\" y=\"-351.8\">decoder_input: InputLayer</text>\n",
       "<polyline fill=\"none\" points=\"174.5,-332.5 174.5,-378.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"202\" y=\"-363.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"174.5,-355.5 229.5,-355.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"202\" y=\"-340.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"229.5,-332.5 229.5,-378.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"275.5\" y=\"-363.3\">(None, None)</text>\n",
       "<polyline fill=\"none\" points=\"229.5,-355.5 321.5,-355.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"275.5\" y=\"-340.3\">(None, None)</text>\n",
       "</g>\n",
       "<!-- 139727418716400 -->\n",
       "<g class=\"node\" id=\"node2\"><title>139727418716400</title>\n",
       "<polygon fill=\"none\" points=\"28,-249.5 28,-295.5 307,-295.5 307,-249.5 28,-249.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"94\" y=\"-268.8\">masking_2: Masking</text>\n",
       "<polyline fill=\"none\" points=\"160,-249.5 160,-295.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"187.5\" y=\"-280.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"160,-272.5 215,-272.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"187.5\" y=\"-257.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"215,-249.5 215,-295.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"261\" y=\"-280.3\">(None, None)</text>\n",
       "<polyline fill=\"none\" points=\"215,-272.5 307,-272.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"261\" y=\"-257.3\">(None, None)</text>\n",
       "</g>\n",
       "<!-- 139727418716568&#45;&gt;139727418716400 -->\n",
       "<g class=\"edge\" id=\"edge1\"><title>139727418716568-&gt;139727418716400</title>\n",
       "<path d=\"M167.5,-332.366C167.5,-324.152 167.5,-314.658 167.5,-305.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"171,-305.607 167.5,-295.607 164,-305.607 171,-305.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 139727418717856 -->\n",
       "<g class=\"node\" id=\"node3\"><title>139727418717856</title>\n",
       "<polygon fill=\"none\" points=\"0,-166.5 0,-212.5 335,-212.5 335,-166.5 0,-166.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"80.5\" y=\"-185.8\">embedding_2: Embedding</text>\n",
       "<polyline fill=\"none\" points=\"161,-166.5 161,-212.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"188.5\" y=\"-197.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"161,-189.5 216,-189.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"188.5\" y=\"-174.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"216,-166.5 216,-212.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"275.5\" y=\"-197.3\">(None, None)</text>\n",
       "<polyline fill=\"none\" points=\"216,-189.5 335,-189.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"275.5\" y=\"-174.3\">(None, None, 100)</text>\n",
       "</g>\n",
       "<!-- 139727418716400&#45;&gt;139727418717856 -->\n",
       "<g class=\"edge\" id=\"edge2\"><title>139727418716400-&gt;139727418717856</title>\n",
       "<path d=\"M167.5,-249.366C167.5,-241.152 167.5,-231.658 167.5,-222.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"171,-222.607 167.5,-212.607 164,-222.607 171,-222.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 139727418716680 -->\n",
       "<g class=\"node\" id=\"node6\"><title>139727418716680</title>\n",
       "<polygon fill=\"none\" points=\"270.5,-83.5 270.5,-129.5 698.5,-129.5 698.5,-83.5 270.5,-83.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"319.5\" y=\"-102.8\">lstm_1: LSTM</text>\n",
       "<polyline fill=\"none\" points=\"368.5,-83.5 368.5,-129.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"396\" y=\"-114.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"368.5,-106.5 423.5,-106.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"396\" y=\"-91.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"423.5,-83.5 423.5,-129.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"561\" y=\"-114.3\">[(None, None, 100), (None, 256), (None, 256)]</text>\n",
       "<polyline fill=\"none\" points=\"423.5,-106.5 698.5,-106.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"561\" y=\"-91.3\">[(None, None, 256), (None, 256), (None, 256)]</text>\n",
       "</g>\n",
       "<!-- 139727418717856&#45;&gt;139727418716680 -->\n",
       "<g class=\"edge\" id=\"edge3\"><title>139727418717856-&gt;139727418716680</title>\n",
       "<path d=\"M253.703,-166.473C295.375,-155.825 345.533,-143.009 388.43,-132.048\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"389.492,-135.389 398.314,-129.522 387.759,-128.607 389.492,-135.389\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 139730996883184 -->\n",
       "<g class=\"node\" id=\"node4\"><title>139730996883184</title>\n",
       "<polygon fill=\"none\" points=\"353,-166.5 353,-212.5 616,-212.5 616,-166.5 353,-166.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"415.5\" y=\"-185.8\">input_1: InputLayer</text>\n",
       "<polyline fill=\"none\" points=\"478,-166.5 478,-212.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"505.5\" y=\"-197.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"478,-189.5 533,-189.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"505.5\" y=\"-174.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"533,-166.5 533,-212.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"574.5\" y=\"-197.3\">(None, 256)</text>\n",
       "<polyline fill=\"none\" points=\"533,-189.5 616,-189.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"574.5\" y=\"-174.3\">(None, 256)</text>\n",
       "</g>\n",
       "<!-- 139730996883184&#45;&gt;139727418716680 -->\n",
       "<g class=\"edge\" id=\"edge4\"><title>139730996883184-&gt;139727418716680</title>\n",
       "<path d=\"M484.5,-166.366C484.5,-158.152 484.5,-148.658 484.5,-139.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"488,-139.607 484.5,-129.607 481,-139.607 488,-139.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 139730996883240 -->\n",
       "<g class=\"node\" id=\"node5\"><title>139730996883240</title>\n",
       "<polygon fill=\"none\" points=\"634,-166.5 634,-212.5 897,-212.5 897,-166.5 634,-166.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"696.5\" y=\"-185.8\">input_2: InputLayer</text>\n",
       "<polyline fill=\"none\" points=\"759,-166.5 759,-212.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"786.5\" y=\"-197.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"759,-189.5 814,-189.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"786.5\" y=\"-174.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"814,-166.5 814,-212.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"855.5\" y=\"-197.3\">(None, 256)</text>\n",
       "<polyline fill=\"none\" points=\"814,-189.5 897,-189.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"855.5\" y=\"-174.3\">(None, 256)</text>\n",
       "</g>\n",
       "<!-- 139730996883240&#45;&gt;139727418716680 -->\n",
       "<g class=\"edge\" id=\"edge5\"><title>139730996883240-&gt;139727418716680</title>\n",
       "<path d=\"M689.087,-166.473C652.536,-155.937 608.62,-143.279 570.863,-132.395\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"571.476,-128.929 560.898,-129.522 569.537,-135.655 571.476,-128.929\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 139727418825080 -->\n",
       "<g class=\"node\" id=\"node7\"><title>139727418825080</title>\n",
       "<polygon fill=\"none\" points=\"346.5,-0.5 346.5,-46.5 622.5,-46.5 622.5,-0.5 346.5,-0.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"397.5\" y=\"-19.8\">dense_1: Dense</text>\n",
       "<polyline fill=\"none\" points=\"448.5,-0.5 448.5,-46.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"476\" y=\"-31.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"448.5,-23.5 503.5,-23.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"476\" y=\"-8.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"503.5,-0.5 503.5,-46.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"563\" y=\"-31.3\">(None, None, 256)</text>\n",
       "<polyline fill=\"none\" points=\"503.5,-23.5 622.5,-23.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"563\" y=\"-8.3\">(None, None, 351)</text>\n",
       "</g>\n",
       "<!-- 139727418716680&#45;&gt;139727418825080 -->\n",
       "<g class=\"edge\" id=\"edge6\"><title>139727418716680-&gt;139727418825080</title>\n",
       "<path d=\"M484.5,-83.3664C484.5,-75.1516 484.5,-65.6579 484.5,-56.7252\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"488,-56.6068 484.5,-46.6068 481,-56.6069 488,-56.6068\" stroke=\"black\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SVG(model_to_dot(decoder_model, show_shapes=True).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def translate_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    h, c = encoder_model.predict(np.reshape(input_seq,(1,max_input_seq_len)))\n",
    "    states_value = [h, c]\n",
    "    \n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1))\n",
    "    # Populate the first word of target sequence with the start symbol '<S>'.\n",
    "    target_seq[0, 0] = word_to_index2['<S>']\n",
    "    \n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    decoded_sentence = ''\n",
    "    stop_condition = False\n",
    "    step = 0\n",
    "    while not stop_condition:\n",
    "        # Use the decoder to get the output token vector and the h and c vectors\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "        \n",
    "        # Find the largest value in the probability output vector, and use that index as your output word\n",
    "        # at this time step.\n",
    "        sampled_token_index = np.argmax(output_tokens)\n",
    "        sampled_word = index_to_word2[sampled_token_index]\n",
    "        \n",
    "        # Add the word to the output sentence string\n",
    "        decoded_sentence += sampled_word + ' '\n",
    "        \n",
    "        # Stopping condition: either hit max length or find the stop token '</S>'.\n",
    "        step += 1\n",
    "        if sampled_word == '</S>' or step == max_seq_len_y:\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "# Test your network: feed in 10 sentences and show the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing sentence 1 of 10\n",
      "<MASK> <MASK> <S> china is usually busy during september , but it is sometimes cold in spring . </S>\n",
      "chine est gnralement occup en septembre , mais il est parfois froid au printemps . </S> \n",
      "\n",
      "Testing sentence 2 of 10\n",
      "<MASK> <MASK> <MASK> <MASK> <MASK> <MASK> <MASK> <MASK> <MASK> <MASK> <MASK> <S> he dislikes pears and peaches . </S>\n",
      "il aime pas les poires et les pches . </S> \n",
      "\n",
      "Testing sentence 3 of 10\n",
      "<MASK> <S> the united states is sometimes rainy during january , but it is mild in may . </S>\n",
      "les tats - unis est parfois pluvieux en janvier , mais il est doux en mai . </S> \n",
      "\n",
      "Testing sentence 4 of 10\n",
      "<MASK> <MASK> <MASK> <S> california is mild during march , but it is sometimes rainy in october . </S>\n",
      "california est doux au mois de mars , mais il est parfois pluvieux en octobre . </S> \n",
      "\n",
      "Testing sentence 5 of 10\n",
      "<MASK> <MASK> <MASK> <MASK> <MASK> <MASK> <MASK> <MASK> <MASK> <MASK> <MASK> <S> he dislikes mangoes and strawberries . </S>\n",
      "il n'aime les mangues et les fraises . </S> \n",
      "\n",
      "Testing sentence 6 of 10\n",
      "<MASK> <MASK> <MASK> <S> the grapefruit is her favorite fruit , but the lemon is your favorite . </S>\n",
      "le pamplemousse est son fruit prfr , mais le citron est votre favori . </S> \n",
      "\n",
      "Testing sentence 7 of 10\n",
      "<MASK> <MASK> <MASK> <MASK> <MASK> <MASK> <MASK> <S> i think translating between english and portuguese is easy . </S>\n",
      "je pense que la traduction entre l' anglais et le portugais est facile . </S> \n",
      "\n",
      "Testing sentence 8 of 10\n",
      "<MASK> <S> your most loved fruit is the apple , but his most loved is the peach . </S>\n",
      "votre fruit le plus aim est la pomme , mais son plus aim est la pche . </S> \n",
      "\n",
      "Testing sentence 9 of 10\n",
      "<MASK> <S> the united states is cold during winter , and it is sometimes relaxing in june . </S>\n",
      "les tats - unis est froid pendant l' hiver , et il est parfois relaxant en juin . </S> \n",
      "\n",
      "Testing sentence 10 of 10\n",
      "<MASK> <S> our most loved fruit is the apple , but my most loved is the pear . </S>\n",
      "nos fruits le plus aim est la pomme , mais mon plus aim est la poire . </S> \n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_to_test = 10\n",
    "for i in range(num_to_test):\n",
    "    print('Testing sentence %i of %i' % (i+1,num_to_test))\n",
    "    # Print an input sentence\n",
    "    english = indices_to_sentence(X_test[i], index_to_word1)\n",
    "    print(' '.join(english))\n",
    "    # Translate it and print the output sentence\n",
    "    translated = translate_sequence(X_test[i])\n",
    "    print(translated)\n",
    "    actual = indices_to_sentence(y_test[i], index_to_word2)\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "### Question: How well do you think the model did? Discuss any problems you ran into."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the 10 sentences fed into the model from the test set, the model appears to be doing a very good job of translating the sentences (I speak some French, and the sentences appear to be mostly correct). This is a promising result, as these are all setences that the model has never seen.\n",
    "\n",
    "The biggest problem I ran into was the fact that I set the 'input_length' argument in the embedding layers. This assumes all input sequences are the same length, which is true during training, and so the model trained successfully. However, during inference the sentences are fed to the decoder a single word at a time, and this led to errors at first (the dense layer was always expecting a 24x310 array, but I was feeding it a 1x310 array). Removing the 'input_length' argument fixed this issue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "# Evaluate model accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "### Compute the accuracy of the model on the test set.\n",
    "In a detailed study we would calculate the \"BLEU\" score for the translation task.\n",
    "For this assignment, we'll keep things simple. Just calculate an all-or-nothing accuracy score on each translated sentence. If all the words appear in the output, in the correct order, without extra words, the score on that example is 1. Otherwise 0. Compute the accuracy over all examples in the test set. You may ignore punctuation (commas) and `<S>` and `</S>` symbols."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translating 41359 test sentences\n",
      "12% complete...\n",
      "24% complete...\n",
      "36% complete...\n",
      "48% complete...\n",
      "60% complete...\n",
      "73% complete...\n",
      "85% complete...\n",
      "97% complete...\n"
     ]
    }
   ],
   "source": [
    "num_correct = 0\n",
    "mistranslated = []\n",
    "print('Translating %i test sentences' % len(X_test))\n",
    "for i in range(len(X_test)):\n",
    "    # Print progress\n",
    "    if (i+1) % 5000 == 0:\n",
    "        print('%2.0f%% complete...' % (i/len(X_test) * 100))\n",
    "    # Translate sentence\n",
    "    translated = translate_sequence(X_test[i])\n",
    "    # Get actual sentence from test response\n",
    "    actual = indices_to_sentence(y_test[i], index_to_word2)\n",
    "    # Remove punctuation + start and stop tokens (plus mask token for actual sentence)\n",
    "    translated_clean = [s for s in translated.split() if s not in ['<S>','</S>',',']]\n",
    "    actual_clean = [s for s in actual if s not in ['<S>','</S>',',','<MASK>']]\n",
    "    # If all entries are equal, increment counter.\n",
    "    # Otherwise, add tuple of sentences to mistranslated array for further review\n",
    "    if translated_clean == actual_clean:\n",
    "        num_correct += 1\n",
    "    else:\n",
    "        mistranslated.append((translated_clean, actual_clean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39919 out of 41359 sentences correctly translated\n",
      "Test set accuracy: 96.52%\n"
     ]
    }
   ],
   "source": [
    "# Calculate test accuracy\n",
    "test_accuracy = num_correct/len(X_test) * 100\n",
    "print('%i out of %i sentences correctly translated' % (num_correct,len(X_test)))\n",
    "print('Test set accuracy: %4.2f%%' % test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "## Report the accuracy value you obtained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The highest test set accuracy obtained was 96.52%, and this is using the entire sample set (nearly 140,000 sentence pairs). Also, the setences were processed in a forward sequence (from the beginning of each sentence to the end, as they would be read). Several trials were done with the sentences in reverse order (using a subset of the data) and it did not appear to have a significant effect on the accuracy of the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at a random sampling of the mistranslated sentences to see how far off the model was."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result  : la france est parfois agrable au cours de l' automne et il est jamais froid en t .\n",
      "Expected: la france est parfois agrable en automne et il est jamais froid en t .\n",
      "\n",
      "Result  : l' inde est gnralement froid au mois de mars mais il est gnralement enneige en juillet .\n",
      "Expected: l' inde est gnralement froid en mars mais il est gnralement enneige en juillet .\n",
      "\n",
      "Result  : ils prvoient de visiter la californie l' automne prochain .\n",
      "Expected: ils ont l' intention de visiter la californie l' automne prochain .\n",
      "\n",
      "Result  : new jersey est agrable en hiver mais jamais de neige en dcembre .\n",
      "Expected: new jersey est agrable pendant l' hiver mais jamais de neige en dcembre .\n",
      "\n",
      "Result  : ils ont l' intention de visiter la france en mai prochain .\n",
      "Expected: ils prvoient de visiter la france en mai prochain .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Grab 5 random mistranslations\n",
    "ind = np.random.choice(list(range(len(mistranslated))), size=5)\n",
    "for i in ind:\n",
    "    print('Result  : %s' % ' '.join(mistranslated[i][0]))\n",
    "    print('Expected: %s' % ' '.join(mistranslated[i][1]))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In each case, the majority of the sentence has been translated correctly, only a word or two is incorrect. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Conceptual Overview\n",
    "\n",
    "The structure of the network contains two parts, one for training and one for inference. Both parts consist of a separate encoder and decoder. For the training portion, English sentences, represented by pre-padded lists of indices corresponding to words in a dictionary, are fed to an embedding layer in the encoder. The embedded word vectors are passed to an LSTM layer in which the internal and long term states (denoted h and c, respectively), are retained. The output from this layer is discarded. At the same time, French sentences, represented by post-padded lists of indices corresponding to words in a dictionary, are fed to an embedding layer in the decoder. The embedded word vectors are passed to an LSTM layer in which the initial state is the state vector returned by the encoder. The output of this layer is passed to a dense layer, where a prediction of the next word in the translated sentence is returned using a softmax activation.\n",
    "\n",
    "The purpose of network is to translate an English sentence to the equivalent sentence in French. The network training process uses what is known as 'teacher forcing', where the model is forced to learn the target output offset by one time step into the future. In other words, during training, rather than being fed the output from the previous timestep (as is typical with RNNs), the output is the 'ideal' output, which is the correct next word in the sentence. This helps prevent mistakes made early on from propagating long term.\n",
    "\n",
    "So far, only the training process has been described. What about when a sentence is going to be translated? Here, a separate network is built using specific parts of the training network. The encoder uses the same input layer and encoder states as the training encoder. The decoder is slightly more complicated. It consists of an LSTM using the input layer from the training decoder and an initial state that will be set to the internal and long term states from an LSTM output. The output of this LSTM layer is fed to a dense layer that predicts the next word in the translated French sentence using a softmax activation as in the training process.\n",
    "\n",
    "The actual translation process occurs as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
