{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cart-Pole Balancing using DQNs\n",
    "In this assignment we will balance a cartpole using deep learning. We will build an agent, that given the current state of the environment, can make a prediction about what action would result in the best outcome. We are going to implement the two core pieces of DQNs, the epsilon greedy algorithm and memory replay. \n",
    "\n",
    "In this assignment we will use openai gym libraries to set up the game enviroment. Most of the game playing interface is already provided by the gym library. Our task is to implement the agent, and fix up the training. As we play the game, you should see the agent's score increase in the training loop. A score of 100 or above is what we are trying to achieve. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "only teacher can use pip3\r\n"
     ]
    }
   ],
   "source": [
    "# If you are running this practice on your machine, make sure to install gym and gym[atari]. Depending on your python \n",
    "# env, this could be done using pip install, or conda install etc. \n",
    "!pip3 install --user gym gym[atari]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "Using TensorFlow backend.\n",
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "from collections import deque\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import gym\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class below creates a deep q-network (DQN) with a specific architecture, along with the relevant parameters (epsilon, gamma, etc.) In addition, it sets up the remember, act, and replay methods, which are required during the training phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.gamma = 0.95   # discount rate\n",
    "        self.epsilon = 1.0  # exploration rate\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.learning_rate = 0.001\n",
    "        self.model = self._build_model()\n",
    "\n",
    "    def _build_model(self):\n",
    "        model = Sequential()        \n",
    "        # Input layer has dimension of self.state_size\n",
    "        model.add(Dense(16, activation='relu', input_shape=(self.state_size,)))\n",
    "        # Intermediate layer\n",
    "        model.add(Dense(16, activation='relu'))\n",
    "        # Output layer has dimension of self.action_size\n",
    "        model.add(Dense(self.action_size, activation='linear'))\n",
    "        # Compile model\n",
    "        optimizer = Adam(lr=self.learning_rate)\n",
    "        model.compile(optimizer=optimizer, loss='mse')\n",
    "        \n",
    "        return model\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        # Add tuple to queue\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):\n",
    "        # In this function we calculate and return the next action.\n",
    "        # We are going to implement epsilon greedy logic here. \n",
    "        # With probability epsilon, return a random action and return that\n",
    "        # With probability 1-epsilon return the action that the model predicts. \n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            # Return random action\n",
    "            return env.action_space.sample()\n",
    "        else:\n",
    "            # Return predicted action\n",
    "            act_values = self.model.predict(state)\n",
    "            return np.argmax(act_values[0])\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        # We'll sample from our memories and get a handful of them and store them in minibatch \n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            target = reward\n",
    "            if not done:\n",
    "                # Calculate the total discounted reward according to the Q-Learning formula\n",
    "                # target = current_reward + discounted maximum value obtained by next state\n",
    "                target = reward + self.gamma * np.amax(self.model.predict(next_state)[0])\n",
    "\n",
    "            target_f = self.model.predict(state)\n",
    "            target_f[0][action] = target\n",
    "            self.model.fit(state, target_f, epochs=1, verbose=0)\n",
    "            \n",
    "        # Decay the epsilon value \n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    def load(self, name):\n",
    "        self.model.load_weights(name)\n",
    "\n",
    "    def save(self, name):\n",
    "        self.model.save_weights(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below creates an environment for our game, an instance of a DQNAgent object, and proceeds to train the model to play the game over a series of 40 episodes. At each episode, the game proceeds for as long as the pole is able to remain upright. This is reflected in the 'time' variable, which is the for loop iterator.\n",
    "\n",
    "Early on, the actions taken by the agent are mostly random (this corresponds to high values of epsilon). Over time, the value of epsilon decreases, and the actions taken by the agent are predicted by the DQN (they become more exploitative, and less explorative). In theory, since the model has been trained for a sufficient length of time, these actions should be advantageous, in other words they should contribute to keeping the pole upright and letting the game play for longer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "State size: 4\n",
      "Action size: 2\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_12 (Dense)             (None, 16)                80        \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 2)                 34        \n",
      "=================================================================\n",
      "Total params: 386\n",
      "Trainable params: 386\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "episode: 1/40, score: 28, eps: 1.0\n",
      "episode: 2/40, score: 17, eps: 0.93\n",
      "episode: 3/40, score: 34, eps: 0.79\n",
      "episode: 4/40, score: 14, eps: 0.73\n",
      "episode: 5/40, score: 12, eps: 0.69\n",
      "episode: 6/40, score: 9, eps: 0.66\n",
      "episode: 7/40, score: 21, eps: 0.59\n",
      "episode: 8/40, score: 14, eps: 0.55\n",
      "episode: 9/40, score: 18, eps: 0.51\n",
      "episode: 10/40, score: 15, eps: 0.47\n",
      "episode: 11/40, score: 11, eps: 0.44\n",
      "episode: 12/40, score: 13, eps: 0.42\n",
      "episode: 13/40, score: 10, eps: 0.4\n",
      "episode: 14/40, score: 9, eps: 0.38\n",
      "episode: 15/40, score: 9, eps: 0.36\n",
      "episode: 16/40, score: 22, eps: 0.32\n",
      "episode: 17/40, score: 18, eps: 0.3\n",
      "episode: 18/40, score: 13, eps: 0.28\n",
      "episode: 19/40, score: 13, eps: 0.26\n",
      "episode: 20/40, score: 21, eps: 0.23\n",
      "episode: 21/40, score: 20, eps: 0.21\n",
      "episode: 22/40, score: 26, eps: 0.19\n",
      "episode: 23/40, score: 80, eps: 0.12\n",
      "episode: 24/40, score: 54, eps: 0.095\n",
      "episode: 25/40, score: 66, eps: 0.068\n",
      "episode: 26/40, score: 37, eps: 0.057\n",
      "episode: 27/40, score: 146, eps: 0.027\n",
      "episode: 28/40, score: 74, eps: 0.019\n",
      "episode: 29/40, score: 57, eps: 0.014\n",
      "episode: 30/40, score: 51, eps: 0.011\n",
      "episode: 31/40, score: 46, eps: 0.01\n",
      "episode: 32/40, score: 58, eps: 0.01\n",
      "episode: 33/40, score: 59, eps: 0.01\n",
      "episode: 34/40, score: 117, eps: 0.01\n",
      "episode: 35/40, score: 159, eps: 0.01\n",
      "episode: 36/40, score: 274, eps: 0.01\n",
      "episode: 37/40, score: 249, eps: 0.01\n",
      "episode: 38/40, score: 180, eps: 0.01\n",
      "episode: 39/40, score: 268, eps: 0.01\n",
      "episode: 40/40, score: 189, eps: 0.01\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    env = gym.make('CartPole-v1')\n",
    "    \n",
    "    # State size for CartPole game\n",
    "    state_size = env.observation_space.shape[0]\n",
    "    # Action size for CartPole game\n",
    "    action_size = env.action_space.n\n",
    "    \n",
    "    print('State size: %i' % state_size)\n",
    "    print('Action size: %i' % action_size)\n",
    "    \n",
    "    agent = DQNAgent(state_size, action_size)\n",
    "    agent.model.summary()\n",
    "    done = False\n",
    "    batch_size = 32 # Feel free to play with these \n",
    "    EPISODES = 40   # You shouldn't really need more than 100 episodes to get a score of 100\n",
    "\n",
    "    \n",
    "    for eps in range(1,EPISODES+1):\n",
    "        state = env.reset()\n",
    "        state = np.reshape(state, [1, state_size])\n",
    "        for time in range(500):\n",
    "            \n",
    "            # Get an action from the agent\n",
    "            action = agent.act(state)\n",
    "            # Send this action to the env and get the next_state, reward, done values\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            \n",
    "            # DO NOT CHANGE THE FOLLOWING 2 LINES \n",
    "            reward = reward if not done else -10\n",
    "            next_state = np.reshape(next_state, [1, state_size])\n",
    "            \n",
    "            # Tell the agent to remember this memory\n",
    "            agent.remember(state, action, reward, next_state, done)\n",
    "            \n",
    "            # DO NOT CHANGE BELOW THIS LINE\n",
    "            state = next_state\n",
    "            if done:\n",
    "                print(\"episode: {}/{}, score: {}, eps: {:.2}\".format(eps, EPISODES, time, agent.epsilon))\n",
    "                break\n",
    "            if len(agent.memory) > batch_size:\n",
    "                agent.replay(batch_size)\n",
    "        if eps % 10 == 0:\n",
    "            agent.save(\"./cartpole-dqn.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The agent was successful, and the desired score of 100 was consistently reached by the end of the 40 training episodes. \n",
    "\n",
    "It is worth noting that the agent doesn't always succeed, sometimes the desired score of 100 is not achieved. I was curious about the success rate of the agent, so I set up a simple trial below. The code above is repeated 10 times, and the success rate of the agent being able to reach 100 points is tracked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "Starting trial 0...\n",
      "Score of at least 100 achieved at episode 17\n",
      "Starting trial 1...\n",
      "Maximum episodes reached, no high score...\n",
      "Starting trial 2...\n",
      "Score of at least 100 achieved at episode 16\n",
      "Starting trial 3...\n",
      "Maximum episodes reached, no high score...\n",
      "Starting trial 4...\n",
      "Score of at least 100 achieved at episode 35\n",
      "Starting trial 5...\n",
      "Score of at least 100 achieved at episode 27\n",
      "Starting trial 6...\n",
      "Score of at least 100 achieved at episode 11\n",
      "Starting trial 7...\n",
      "Score of at least 100 achieved at episode 26\n",
      "Starting trial 8...\n",
      "Score of at least 100 achieved at episode 28\n",
      "Starting trial 9...\n",
      "Score of at least 100 achieved at episode 32\n",
      "Percent successful trials: 80.00\n"
     ]
    }
   ],
   "source": [
    "success = 0\n",
    "n = 10\n",
    "env = gym.make('CartPole-v1')\n",
    "for i in range(n):\n",
    "    print('Starting trial %i...' % i)\n",
    "\n",
    "    # State size for CartPole game\n",
    "    state_size = env.observation_space.shape[0]\n",
    "    # Action size for CartPole game\n",
    "    action_size = env.action_space.n\n",
    "\n",
    "    agent = DQNAgent(state_size, action_size)\n",
    "    done = False\n",
    "    batch_size = 32 # Feel free to play with these \n",
    "    EPISODES = 40   # You shouldn't really need more than 100 episodes to get a score of 100\n",
    "\n",
    "    proceed = True\n",
    "    for eps in range(1,EPISODES+1):\n",
    "        if proceed:\n",
    "            state = env.reset()\n",
    "            state = np.reshape(state, [1, state_size])\n",
    "            for time in range(500):\n",
    "\n",
    "                # Get an action from the agent\n",
    "                action = agent.act(state)\n",
    "                # Send this action to the env and get the next_state, reward, done values\n",
    "                next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "                # DO NOT CHANGE THE FOLLOWING 2 LINES \n",
    "                reward = reward if not done else -10\n",
    "                next_state = np.reshape(next_state, [1, state_size])\n",
    "\n",
    "                # Tell the agent to remember this memory\n",
    "                agent.remember(state, action, reward, next_state, done)\n",
    "\n",
    "                # DO NOT CHANGE BELOW THIS LINE\n",
    "                state = next_state\n",
    "                if done:\n",
    "                    if time >= 100:\n",
    "                        print('Score of at least 100 achieved at episode %i' % eps)\n",
    "                        success += 1\n",
    "                        proceed = False\n",
    "                    break\n",
    "                if len(agent.memory) > batch_size:\n",
    "                    agent.replay(batch_size)\n",
    "        else:\n",
    "            break\n",
    "            \n",
    "    if eps == EPISODES:\n",
    "        print('Maximum episodes reached, no high score...')\n",
    "\n",
    "print('Percent successful trials: %4.2f' % (success/n * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "It appears that 8 out of 10, or 80% of the trials resulted in a successful outcome. That is, much more often than not, the agent is able to score at least 100 points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Conceptual Overview\n",
    "\n",
    "The structure of the DQN created in this assignment is very straightforward. It contains an input layer with a node for each of the observation states. In this case there are four observations:\n",
    "* The position of the cart\n",
    "* The velocity of the cart\n",
    "* The angle of the pole\n",
    "* The rotation rate of the pole\n",
    "\n",
    "After this, there are two dense layers of 16 nodes each, and each layer uses a relu activation. The specific number of nodes was determined through trial and error, using 16 nodes let to the highest success rate for the agent (at 80%, as described above).\n",
    "\n",
    "The output layer contains a node for each of the actions that the agent can take. In this case there are only two actions:\n",
    "* Move the cart to the left\n",
    "* Move the cart to the right\n",
    "\n",
    "The output layer uses a linear activation function, and the action with the largest value is taken as the chosen action.\n",
    "\n",
    "The purpose of the network is to predict the action most likely to result in a reward (i.e., keep the pole upright), given the current state of the observation space. \n",
    "\n",
    "The training of network proceeds as follows: First, an action is sample from the agent. Early on, this action is most likely random, but over time, the network itself is used to predict the best action given the current environment. The value of the epsilon parameter controls the probability of taking a random versus predicted action, and this probability decreases over time. The chosen action is taken, and the next state, reward, and whether or not the game has finished are returned by the environment. This entire series of events is remembered by the agent as a 'memory'.  If the game is not finished, and enough memories have been stored, these memories are replayed to train the DQN, making it 'better' at playing the game over time. It is important to note that during this replay phase, a discounted reward is used to prioritize rewards in the somewhat distant future (it's advantageous for the agent to keep the pole upright for a long time, so it's good to focus on long term rewards).\n",
    "\n",
    "Overall, I was pleasantly surprised that a simple network could be used to successfully play the CartPole game. The entire DQN had only 386 weights to train, which is miniscule compared to most networks we've seen in this course. I arrived at the network structure using trial and error, I found that using 8 or 24 nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
