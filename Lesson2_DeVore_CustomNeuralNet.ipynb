{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "# Lesson 2: Neural Net Computations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "In the previous assignment we used Keras to train a neural network. In this assignment you will build your own minimal neural net library. The basic structure is given to you; you will need to fill in details such as weight updating for backpropogation. Then you will test the network on learning the XOR function.\n",
    "\n",
    "Read through the class definitions below first to understand the basic architecture.\n",
    "\n",
    "Then you should add code as necessary where marked \"TODO\" in the code below and remove the NotImplementedError exceptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "## Define a Neural Network Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NNet():\n",
    "    \"\"\"Implements a basic feedforward neural network.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self._layers = []  # An ordered list of layers. The first layer is the input; the final is the output.\n",
    "    \n",
    "    def _add_layer(self, layer):\n",
    "        if self._layers:\n",
    "            # Update pointers. We keep a doubly-linked-list of layers for convenience.\n",
    "            prev_layer = self._layers[-1]\n",
    "            prev_layer.set_next_layer(layer)\n",
    "            layer.set_prev_layer(prev_layer)\n",
    "            \n",
    "        self._layers.append(layer)\n",
    "    \n",
    "    def add_input_layer(self, size, **kwargs):\n",
    "        assert type(size).__name__ == 'int', ('Input layer requires integer size. Type was %s instead.' \n",
    "                                              % type(size).__name__)\n",
    "        layer = InputLayer(size=size, **kwargs)\n",
    "        self._add_layer(layer)\n",
    "\n",
    "    def add_dense_layer(self, size, **kwargs):\n",
    "        assert type(size).__name__ == 'int', ('Dense layer requires integer size. Type was %s instead.' \n",
    "                                              % type(size).__name__)\n",
    "        # Find the previous layer's size.\n",
    "        prev_size = self._layers[-1].size()\n",
    "        layer = DenseLayer(shape=(prev_size, size), **kwargs)\n",
    "        self._add_layer(layer)\n",
    "\n",
    "    def summary(self, verbose=False):\n",
    "        \"\"\"Prints a description of the model.\"\"\"\n",
    "        for i, layer in enumerate(self._layers):\n",
    "            print('%d: %s' % (i, str(layer)))\n",
    "            if verbose:\n",
    "                print('weights:', layer.get_weights())\n",
    "                if layer._use_bias:\n",
    "                    print('bias:', layer._bias)\n",
    "                print()\n",
    "\n",
    "        \n",
    "    def predict(self, x):\n",
    "        \"\"\"Given an input vector x, run it through the neural network and return the output vector.\"\"\"\n",
    "        assert isinstance(x, np.ndarray)\n",
    "        \n",
    "        # Here, x is a vector corresponding to the ith row of the X_data matrix\n",
    "        # To run x through the network, iterate over the layers, passing the output of the current layer\n",
    "        # as input to the next layer\n",
    "        inputs = x\n",
    "        for i,layer in enumerate(self._layers):\n",
    "            # Pass inputs through current layer\n",
    "            outputs = layer.feed_forward(inputs)          \n",
    "            # If more layers to go, set outputs to be inputs for next layer\n",
    "            if layer != self._layers[-1]:\n",
    "                inputs = outputs          \n",
    "        \n",
    "        return outputs\n",
    "        \n",
    "        \n",
    "    def train_single_example(self, X_data, y_data, learning_rate=0.01):\n",
    "        \"\"\"Train on a single example. X_data and y_data must be numpy arrays.\"\"\"\n",
    "        \n",
    "        assert isinstance(X_data, np.ndarray)\n",
    "        assert isinstance(y_data, np.ndarray)\n",
    "\n",
    "        # Forward propagation.\n",
    "        # Here, we need to get the current output using the predict() method\n",
    "        output = self.predict(X_data)\n",
    "            \n",
    "        # Compare output to y_data\n",
    "        error = output - y_data\n",
    "        \n",
    "        # Backpropagation.\n",
    "        # Here, we need to walk back through each layer, computing and returning the deltas,\n",
    "        # which get passed to the next layer upstream\n",
    "        for layer in list(reversed(self._layers)):\n",
    "            # Update weights for current layer and return deltas\n",
    "            deltas = layer.backpropagate(error, learning_rate)\n",
    "            # Deltas become error for next layer\n",
    "            error = deltas\n",
    "\n",
    "\n",
    "    def train(self, X_data, y_data, learning_rate, num_epochs, randomize=True, verbose=True, print_every_n=100):\n",
    "        \"\"\"Both X_data and y_data should be ndarrays. One example per row.\n",
    "        \n",
    "        This function takes the data and learning rate, and trains the network for num_epochs passes over the \n",
    "        complete data set. \n",
    "        \n",
    "        If randomize==True, the X_data and y_data should be randomized at the start of each epoch. Of course,\n",
    "        matching X,y pairs should have matching indices after randomization, to avoid scrambling the dataset.\n",
    "        (E.g., a set of indices should be randomized once and then applied to both X and y data.)\n",
    "        \n",
    "        If verbose==True, will print a status report every print_every_n epochs with these\n",
    "        results:\n",
    "        \n",
    "        * Results of running \"predict\" on each example in the training set\n",
    "        * MSE (mean squared error) on the dataset\n",
    "        * Accuracy on the dataset\n",
    "        \"\"\"\n",
    "        assert isinstance(X_data, np.ndarray)\n",
    "        assert isinstance(y_data, np.ndarray)\n",
    "        assert X_data.shape[0] == y_data.shape[0]\n",
    "\n",
    "        for e in range(1,num_epochs+1):\n",
    "            # If verbose is true, print results if epoch mod print_every_n is zero\n",
    "            if verbose and e % print_every_n == 0:\n",
    "                print('At epoch %i, will print results' % e)\n",
    "                print_results = True\n",
    "            else:\n",
    "                print_results = False\n",
    "            # If randomization is requested, shuffle indices\n",
    "            # Otherwise, just use X and y as is\n",
    "            if randomize:\n",
    "                ind = list(range(X_data.shape[0]))\n",
    "                # Numpy random shuffle method works in place\n",
    "                np.random.shuffle(ind)\n",
    "                X_data_e = X_data[ind]\n",
    "                y_data_e = y_data[ind]\n",
    "            else:\n",
    "                X_data_e = X_data\n",
    "                y_data_e = y_data\n",
    "\n",
    "            # Iterate over each row in X_data\n",
    "            for i in range(X_data_e.shape[0]):\n",
    "                # Make sure and use 'epoch' version, which may or may not be scrambled\n",
    "                x = X_data_e[i,:]\n",
    "                y = y_data_e[i]\n",
    "\n",
    "                # Print prediction results if applicable\n",
    "                if print_results:\n",
    "                    print('  Result of calling predict with x = %s -> %f' % (x, self.predict(x)))\n",
    "\n",
    "                # Train network using current row and target\n",
    "                self.train_single_example(x,y,learning_rate)\n",
    "            \n",
    "            # After training on entire data set, print MSE and accuracy\n",
    "            if print_results:\n",
    "                print('  MSE = %f' % self.compute_mean_squared_error(X_data, y_data))\n",
    "                print('  Accuracy = %f' % self.compute_accuracy(X_data, y_data))\n",
    "        \n",
    "    \n",
    "    def compute_mean_squared_error(self, X_data, y_data):\n",
    "        \"\"\"Given input X_data and target y_data, compute and return the mean squared error.\"\"\"\n",
    "        assert isinstance(X_data, np.ndarray)\n",
    "        assert isinstance(y_data, np.ndarray)\n",
    "        assert X_data.shape[0] == y_data.shape[0]\n",
    "        \n",
    "        mse = 0\n",
    "        # MSE is sum of squared deltas between true and predicted values, divided by the number of samples\n",
    "        for i in range(X_data.shape[0]):\n",
    "            x = X_data[i,:]\n",
    "            output = self.predict(x)\n",
    "            mse += (y_data[i] - output)**2\n",
    "\n",
    "        # Divide by number of samples\n",
    "        mse /= X_data.shape[0]\n",
    "        \n",
    "        return mse\n",
    "    \n",
    "    def compute_accuracy(self, X_data, y_data):\n",
    "        \"\"\"Given input X_data and target y_data, convert outputs to binary using a threshold of 0.5\n",
    "        and return the accuracy: # examples correct / total # examples.\"\"\"\n",
    "        assert isinstance(X_data, np.ndarray)\n",
    "        assert isinstance(y_data, np.ndarray)\n",
    "        assert X_data.shape[0] == y_data.shape[0]\n",
    "        \n",
    "        correct = 0\n",
    "        for i in range(len(X_data)):\n",
    "            outputs = self.predict(X_data[i])\n",
    "            outputs = outputs > 0.5\n",
    "            if outputs == y_data[i]:\n",
    "                correct += 1\n",
    "        acc = float(correct) / len(X_data)\n",
    "        return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "## Define activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "class Activation():  # Do not edit; update derived classes.\n",
    "    \"\"\"Base class that represents an activation function and knows how to take its own derivative.\"\"\"\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "    \n",
    "    def activate(x):\n",
    "        \"\"\"x is a scalar or a numpy array. Returns the output y, the result of applying the function to input x.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    def derivative_given_y(self, y):\n",
    "        \"\"\"y is a scalar or a numpy array. \n",
    "        \n",
    "        Returns the derivative d(f)/dx given the *activation* value y.\"\"\"\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class IdentityActivation(Activation):\n",
    "    \"\"\"Activation function that passes input through unchanged.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__(name='Identity')\n",
    "    \n",
    "    def activate(self, x):\n",
    "        \"\"\"x is a scalar or a numpy array. Returns the output y, the result of applying the function to input x.\"\"\"\n",
    "        return x\n",
    "    \n",
    "    def derivative_given_y(self, y):\n",
    "        \"\"\"y is a scalar or a numpy array. \n",
    "        \n",
    "        Returns the derivative d(f)/dx given the *activation* value y.\"\"\"\n",
    "        return 1\n",
    "    \n",
    "    \n",
    "class SigmoidActivation(Activation):\n",
    "    \"\"\"Sigmoid activation function.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__(name='Sigmoid')\n",
    "    \n",
    "    def activate(self, x):\n",
    "        \"\"\"x is a scalar or a numpy array. Returns the output y, the result of applying the function to input x.\"\"\"\n",
    "\n",
    "        # Return input passed through sigmoid activation (y = 1/(1+e^(-x)))\n",
    "        return 1/(1+np.exp(-x))\n",
    "    \n",
    "    def derivative_given_y(self, y):\n",
    "        \"\"\"y is a scalar or a numpy array. \n",
    "        \n",
    "        Returns the derivative d(f)/dx given the *activation* value y.\"\"\"\n",
    "        # Return derivative of sigmoid function, if y = simga(z), y' = y(1-y)\n",
    "        return y*(1-y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "## Define a method to initialize neural net weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def WeightInitializer():\n",
    "    \"\"\"Function to return a random weight. for example, return a random float from -1 to 1.\"\"\"\n",
    "    # Return a random sample from the uniform distribution [0,1)\n",
    "    weight = np.random.random()\n",
    "    # Create another random variable to assign sign, expands random interval to (-1,1)\n",
    "    sign = np.random.random()\n",
    "    if sign < 0.5:\n",
    "        weight = -weight\n",
    "        \n",
    "    return weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "## Define a neural net Layer base class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "class Layer():\n",
    "    \"\"\"Base class for NNet layers. DO NOT MODIFY THIS CLASS. Update derived classes instead.\n",
    "    \n",
    "    Conceptually, in this library a Layer consists at a high level of:\n",
    "      * a collection of weights (a 2D numpy array)\n",
    "      * the output nodes that come after the weights above\n",
    "      * the activation function that is applied to the summed signals in these output nodes\n",
    "      \n",
    "    So a Layer isn't just nodes -- it's weights as well as nodes.\n",
    "      \n",
    "    Specifically, to send signal forward through a 3-layer network, we start with an Input Layer that does\n",
    "    very little.  The outputs from the Input layer are simply the fed-in input data.  \n",
    "    \n",
    "    Then, the next layer will be a Dense layer that holds the weights from the Input layer to the first hidden\n",
    "    layer and stores the activation function to be used after doing a product of weights and Input-Layer\n",
    "    outputs.\n",
    "    \n",
    "    Finally, another Dense layer will hold the weights from the hidden to the output layer nodes, and stores\n",
    "    the activation function to be applied to the final output nodes.\n",
    "    \n",
    "    For a typical 1-hidden layer network, then, we would have 1 Input layer and 2 Dense layers.\n",
    "    \n",
    "    Each Layer also has funcitons to perform the forward-pass and backpropagation steps for the weights/nodes\n",
    "    associated with the layer.\n",
    "    \n",
    "    Finally, each Layer stores pointers to the pervious and next layers, for convenience when implementing\n",
    "    backprop.\n",
    "    \"\"\"\n",
    "   \n",
    "    def __init__(self, shape, use_bias, activation_function=IdentityActivation, weight_initializer=None, name=''):\n",
    "        # These are the weights from the *previous* layer to the current layer.\n",
    "        self._weights = None\n",
    "        \n",
    "        # Tuple of (# inputs, # outputs) for Dense layers or just a scalar for an input layer.\n",
    "        assert type(shape).__name__ == 'int' or type(shape).__name__ == 'tuple', (\n",
    "            'shape must be scalar or a 2-element tuple')\n",
    "        if type(shape).__name__ == 'tuple':\n",
    "            assert len(shape)==2, 'shape must be 2-dimensional. Was %d instead' % len(shape)\n",
    "        self._shape = shape \n",
    "    \n",
    "        # True to use a bias node that inputs to each node in this layer; False otherwise.\n",
    "        self._use_bias = use_bias\n",
    "        \n",
    "        if use_bias:\n",
    "            bias_size = shape[-1] if len(shape) > 1 else shape\n",
    "            self._bias = np.zeros(bias_size)\n",
    "            if weight_initializer:\n",
    "                for i in range(bias_size):\n",
    "                    self._bias[i] = weight_initializer()\n",
    "        \n",
    "        # Activation function to be applied to each dot product of weights with inputs.\n",
    "        # Instantiate an object of this class.\n",
    "        self._activation_function = activation_function() if activation_function else None\n",
    "        \n",
    "        # Method used to initialize the weights in this Layer at creation time.\n",
    "        self._weight_initializer = weight_initializer\n",
    "        \n",
    "        # Layer name (optional)\n",
    "        self._name = name\n",
    "        \n",
    "        # Calculated output vector from the most recent feed_forward(inputs) call.\n",
    "        self._outputs = None\n",
    "        \n",
    "        # Doubly linked list pointers to neighbor layers.\n",
    "        self._prev_layer = None  # Previous layer is closer to (or is) the input layer.\n",
    "        self._next_layer = None  # Next layer is closer to (or is) the output layer.\n",
    "    \n",
    "    def set_prev_layer(self, layer):\n",
    "        \"\"\"Set pointer to the previous layer.\"\"\"\n",
    "        self._prev_layer = layer\n",
    "    \n",
    "    def set_next_layer(self, layer):\n",
    "        \"\"\"Set pointer to the next layer.\"\"\"\n",
    "        self._next_layer = layer\n",
    "    \n",
    "    def size(self):\n",
    "        \"\"\"Number of nodes in this layer.\"\"\"\n",
    "        if type(self._shape).__name__ == 'tuple':\n",
    "            return self._shape[-1]\n",
    "        else:\n",
    "            return self._shape\n",
    "        \n",
    "    def get_weights(self):\n",
    "        \"\"\"Return a numpy array of the weights for inputs to this layer.\"\"\"\n",
    "        return self._weights\n",
    "    \n",
    "    def get_bias(self):\n",
    "        \"\"\"Return a numpy array of the bias for nodes in this layer.\"\"\"\n",
    "        return self._bias\n",
    "    \n",
    "    def feed_forward(self, inputs):\n",
    "        \"\"\"Feed the given inputs through the input weights and activation function, and set the outputs vector.\n",
    "        \n",
    "        Also returns the outputs vector for convenience.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "    def backpropagate(self, error, learning_rate):\n",
    "        \"\"\"Adjusts the weights coming into this layer based on the given output error vector.\n",
    "        \n",
    "        For the output layer, the \"error\" vector should be a list of output errors, y_k - t_k.\n",
    "        For a hidden layer, the \"error\" vector should be a list of the delta values from the following layer, such as delta_z_k\n",
    "        \n",
    "        Returns a list of the delta values for each node in this layer. These deltas can be used as the error\n",
    "        values when calling backpropagate on the previous layer.\"\"\"\n",
    "        raise NotimplementedError()\n",
    "        \n",
    "    def __str__(self):\n",
    "        activation_fxn_name = self._activation_function.name if self._activation_function else None\n",
    "        return '[%s] shape %s, use_bias=%s, activation=%s' % (self._name, self._shape, self._use_bias,\n",
    "                                                              activation_fxn_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "### Define InputLayer and DenseLayer base classes\n",
    "\n",
    "The DenseLayer class is where most of the computation happens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "class InputLayer(Layer):\n",
    "    \"\"\"A neural network 1-dimensional input layer.\"\"\"\n",
    "    \n",
    "    def __init__(self, size, name='Input'):\n",
    "        assert type(size).__name__ == 'int', 'Input size must be integer. Was %s instead' % type(size).__name__\n",
    "        super().__init__(shape=size, use_bias=False, name=name, activation_function=None)\n",
    "    \n",
    "    def feed_forward(self, inputs):\n",
    "        assert len(inputs)==self._shape, 'Inputs must be of size %d; was %d instead' % (self._shape, len(inputs))\n",
    "        self._outputs = inputs\n",
    "        return self._outputs\n",
    "\n",
    "    def backpropagate(self, error, learning_rate):\n",
    "        return None  # Nothing to do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DenseLayer(Layer):\n",
    "    \"\"\"A neural network layer that is fully connected to the previous layer.\"\"\"\n",
    "    \n",
    "    def __init__(self, shape, use_bias=True, name='Dense', **kwargs):\n",
    "        super().__init__(shape=shape, use_bias=use_bias, name=name, **kwargs)\n",
    "        \n",
    "        self._weights = np.zeros(shape)\n",
    "        if self._weight_initializer:\n",
    "            for i in range(shape[0]):\n",
    "                for j in range(shape[1]):\n",
    "                    self._weights[i,j] = self._weight_initializer()\n",
    "    \n",
    "    def feed_forward(self, inputs):\n",
    "        \"\"\"Feed the given inputs through the input weights and activation function, and set the outputs vector.\n",
    "        \n",
    "        Also returns the outputs vector for convenience.\"\"\"\n",
    "        assert len(inputs)==self._shape[0], 'Inputs must be of size %s; was %s instead' % (self._shape, len(inputs))\n",
    "        \n",
    "        num_nodes = self.size()\n",
    "        \n",
    "        output = np.zeros(num_nodes)\n",
    "        # For each node, compute dot product of input values and corresponding column of weight tensor\n",
    "        # The weight tensor is of size (input nodes, layer nodes)\n",
    "        for i in range(num_nodes):\n",
    "            x = np.dot(inputs,self._weights[:,i])\n",
    "            # Add bias term if applicable\n",
    "            if self._use_bias:\n",
    "                x += self._bias[i]\n",
    "            # Pass through activation function\n",
    "            output[i] = self._activation_function.activate(x)\n",
    "        \n",
    "        # Update output vector for later use, and return it.\n",
    "        self._outputs = output \n",
    "        return self._outputs\n",
    "        \n",
    "    def backpropagate(self, error, learning_rate):\n",
    "        \"\"\"Adjusts the weights coming into this layer based on the given output error vector.\n",
    "        \n",
    "        For the output layer, the \"error\" vector should be a list of output errors, y_k - t_k.\n",
    "        For a hidden layer, the \"error\" vector should be a list of the delta values from the following layer, \n",
    "        such as delta_z_k\n",
    "        \n",
    "        Returns a list of the delta values for each node in this layer. These deltas can be used as the error\n",
    "        values when calling backpropagate on the previous layer.\"\"\"\n",
    "        assert isinstance(error, np.ndarray)\n",
    "        assert isinstance(self._prev_layer._outputs, np.ndarray)\n",
    "        assert isinstance(self._outputs, np.ndarray)  \n",
    "        \n",
    "        # Compute deltas. \n",
    "        deltas = None\n",
    "        # One delta term for each node\n",
    "        num_nodes = self.size()\n",
    "        deltas = np.zeros(num_nodes)\n",
    "        for i in range(num_nodes):\n",
    "            # Differentiate between output layer and general layer\n",
    "            if not self._next_layer:\n",
    "                # For output layer, delta is entry from error vector multiplied by \n",
    "                # the derivative of the activation function given the output of the layer\n",
    "                # For a sigmoid activation function, this is equal to (y_k - t_k)*y_k*(1-y_k)\n",
    "                deltas[i] = error[i] * self._activation_function.derivative_given_y(self._outputs[i])\n",
    "            else:\n",
    "                # For a hidden layer, delta is summation of delta values from following layer multiplied\n",
    "                # by the weights from the current node to each node of that layer, multiplied by\n",
    "                # the derivative of the activation function given the output of the layer\n",
    "                deltas[i] = np.dot(error,self._next_layer._weights[i,:]) * self._activation_function.derivative_given_y(self._outputs[i])\n",
    "                \n",
    "        # Compute gradient.\n",
    "        # The number of terms in the gradient is determined by the number of weights\n",
    "        # Need one gradient per weight term\n",
    "        gradient = np.zeros(self._shape)\n",
    "        for i in range(gradient.shape[0]):\n",
    "            for j in range(gradient.shape[1]):\n",
    "                gradient[i,j] = deltas[j]*self._prev_layer._outputs[i]\n",
    "        \n",
    "        # Adjust weights.\n",
    "        # Subtract gradient multiplied by the learning rate\n",
    "        self._weights -= learning_rate * gradient\n",
    "        \n",
    "        # Adjust bias weights.\n",
    "        if self._use_bias:\n",
    "            # For deltas, inputs are always equal to one so the gradient is just the deltas\n",
    "            # Subtract gradient multiplied by the learning rate\n",
    "            self._bias -= learning_rate * deltas\n",
    "            \n",
    "        return deltas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "# Train a neural net\n",
    "\n",
    "## Create a dataset for the XOR problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [1 1]]\n",
      "[[0]\n",
      " [1]\n",
      " [1]\n",
      " [0]]\n"
     ]
    }
   ],
   "source": [
    "X_data = np.array([[0,0],[1,0],[0,1],[1,1]])\n",
    "y_data = np.array([[0,1,1,0]]).T\n",
    "print(X_data)\n",
    "print(y_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "## Create a neural network using the library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: [Input] shape 2, use_bias=False, activation=None\n",
      "1: [Dense] shape (2, 2), use_bias=True, activation=Sigmoid\n",
      "2: [Output] shape (2, 1), use_bias=True, activation=Sigmoid\n"
     ]
    }
   ],
   "source": [
    "nnet = NNet()\n",
    "nnet.add_input_layer(2)\n",
    "nnet.add_dense_layer(2, weight_initializer=WeightInitializer, activation_function=SigmoidActivation)\n",
    "nnet.add_dense_layer(1, weight_initializer=WeightInitializer, activation_function=SigmoidActivation, name='Output')\n",
    "nnet.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: [Input] shape 2, use_bias=False, activation=None\n",
      "weights: None\n",
      "\n",
      "1: [Dense] shape (2, 2), use_bias=True, activation=Sigmoid\n",
      "weights: [[ 0.02423262 -0.54017482]\n",
      " [-0.33705096 -0.29935796]]\n",
      "bias: [-0.17972313  0.19412596]\n",
      "\n",
      "2: [Output] shape (2, 1), use_bias=True, activation=Sigmoid\n",
      "weights: [[ 0.03471281]\n",
      " [-0.87833632]]\n",
      "bias: [-0.77801281]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nnet.summary(verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "# Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 2000, will print results\n",
      "  Result of calling predict with x = [0 1] -> 0.531558\n",
      "  Result of calling predict with x = [0 0] -> 0.376932\n",
      "  Result of calling predict with x = [1 1] -> 0.573031\n",
      "  Result of calling predict with x = [1 0] -> 0.541426\n",
      "  MSE = 0.223262\n",
      "  Accuracy = 0.750000\n",
      "At epoch 4000, will print results\n",
      "  Result of calling predict with x = [0 1] -> 0.719427\n",
      "  Result of calling predict with x = [1 0] -> 0.728869\n",
      "  Result of calling predict with x = [1 1] -> 0.433323\n",
      "  Result of calling predict with x = [0 0] -> 0.166791\n",
      "  MSE = 0.090874\n",
      "  Accuracy = 1.000000\n",
      "At epoch 6000, will print results\n",
      "  Result of calling predict with x = [1 1] -> 0.135471\n",
      "  Result of calling predict with x = [0 1] -> 0.898208\n",
      "  Result of calling predict with x = [0 0] -> 0.083708\n",
      "  Result of calling predict with x = [1 0] -> 0.898092\n",
      "  MSE = 0.011487\n",
      "  Accuracy = 1.000000\n",
      "At epoch 8000, will print results\n",
      "  Result of calling predict with x = [1 0] -> 0.933282\n",
      "  Result of calling predict with x = [0 1] -> 0.933486\n",
      "  Result of calling predict with x = [0 0] -> 0.058277\n",
      "  Result of calling predict with x = [1 1] -> 0.086384\n",
      "  MSE = 0.004926\n",
      "  Accuracy = 1.000000\n",
      "At epoch 10000, will print results\n",
      "  Result of calling predict with x = [0 0] -> 0.046665\n",
      "  Result of calling predict with x = [1 1] -> 0.066663\n",
      "  Result of calling predict with x = [0 1] -> 0.947763\n",
      "  Result of calling predict with x = [1 0] -> 0.947696\n",
      "  MSE = 0.003019\n",
      "  Accuracy = 1.000000\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.1\n",
    "num_epochs = 10000\n",
    "nnet.train(X_data, y_data, learning_rate, num_epochs, randomize=True, verbose=True, print_every_n=2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "## Print the resuting neural net weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: [Input] shape 2, use_bias=False, activation=None\n",
      "weights: None\n",
      "\n",
      "1: [Dense] shape (2, 2), use_bias=True, activation=Sigmoid\n",
      "weights: [[-3.9338023  -5.69306726]\n",
      " [-3.94033196 -5.72939672]]\n",
      "bias: [5.80457566 2.1527555 ]\n",
      "\n",
      "2: [Output] shape (2, 1), use_bias=True, activation=Sigmoid\n",
      "weights: [[ 7.6353909 ]\n",
      " [-7.96431195]]\n",
      "bias: [-3.49408942]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nnet.summary(verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trials To Establish 100% Convergence Rate\n",
    "\n",
    "Note that the network does not always achieve 100% accuracy on the XOR data set (the same was seen with the Keras library from the last assignment). It is worth exploring the rate at which 100% accuracy is achieved. The following block of code runs creates 100 different models using the same architecture as above and stores the accuracy achieved after each training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_trials(num_hidden_nodes,learning_rate,num_epochs,num_trials):\n",
    "    accuracy = np.zeros(num_trials)\n",
    "    for i in range(num_trials):\n",
    "        if (i+1) % 10 == 0:\n",
    "            print('%3.0f%% of trials complete' % ((i+1)/num_trials * 100))\n",
    "        nnet_trial = NNet()\n",
    "        nnet_trial.add_input_layer(2)\n",
    "        nnet_trial.add_dense_layer(num_hidden_nodes, weight_initializer=WeightInitializer, activation_function=SigmoidActivation)\n",
    "        nnet_trial.add_dense_layer(1, weight_initializer=WeightInitializer, activation_function=SigmoidActivation, name='Output')\n",
    "\n",
    "        nnet_trial.train(X_data, y_data, learning_rate, num_epochs, randomize=True, verbose=False)\n",
    "\n",
    "        accuracy[i] = nnet_trial.compute_accuracy(X_data,y_data)\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 10% of trials complete\n",
      " 20% of trials complete\n",
      " 30% of trials complete\n",
      " 40% of trials complete\n",
      " 50% of trials complete\n",
      " 60% of trials complete\n",
      " 70% of trials complete\n",
      " 80% of trials complete\n",
      " 90% of trials complete\n",
      "100% of trials complete\n"
     ]
    }
   ],
   "source": [
    "# Run trials\n",
    "num_hidden_nodes = 2\n",
    "learning_rate = 0.1\n",
    "num_epochs = 10000\n",
    "num_trials = 100\n",
    "accuracy = run_trials(num_hidden_nodes,learning_rate,num_epochs,num_trials)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below reports the percentage of successes. Typically, the network has shown 70-80% success after repeated trials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using a single hidden layer with 2 nodes, a learning rate of 0.10 and 10000 epochs,\n",
      "72 out of 100 trials (72.0%) achieved perfect classification accuracy for XOR\n"
     ]
    }
   ],
   "source": [
    "success = len(np.where(accuracy == 1)[0])\n",
    "print('Using a single hidden layer with %i nodes, a learning rate of %4.2f and %i epochs,' % (num_hidden_nodes,learning_rate, num_epochs))\n",
    "print('%i out of %i trials (%4.1f%%) achieved perfect classification accuracy for XOR' % (success, num_trials, success/num_trials * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As on final test, it is worth exploring how a change in the architecture affects the convergence rate for 100% accuracy. The code below repeats the trials, this time with three nodes in the hidden layer, rather than two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 10% of trials complete\n",
      " 20% of trials complete\n",
      " 30% of trials complete\n",
      " 40% of trials complete\n",
      " 50% of trials complete\n",
      " 60% of trials complete\n",
      " 70% of trials complete\n",
      " 80% of trials complete\n",
      " 90% of trials complete\n",
      "100% of trials complete\n"
     ]
    }
   ],
   "source": [
    "# Run trials\n",
    "num_hidden_nodes = 3\n",
    "learning_rate = 0.1\n",
    "num_epochs = 10000\n",
    "num_trials = 100\n",
    "accuracy = run_trials(num_hidden_nodes,learning_rate,num_epochs,num_trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using a single hidden layer with 3 nodes, a learning rate of 0.10 and 10000 epochs,\n",
      "98 out of 100 trials (98.0%) achieved perfect classification accuracy for XOR\n"
     ]
    }
   ],
   "source": [
    "success = len(np.where(accuracy == 1)[0])\n",
    "print('Using a single hidden layer with %i nodes, a learning rate of %4.2f and %i epochs,' % (num_hidden_nodes,learning_rate, num_epochs))\n",
    "print('%i out of %i trials (%4.1f%%) achieved perfect classification accuracy for XOR' % (success, num_trials, success/num_trials * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By adding another hidden node, we increased the 100% accuracy rate from 72% to 98%. This is a huge improvement, and it was gained by adding a bit more complexity to the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Conceptual Overview\n",
    "\n",
    "The purpose of this assignment was to complete the creation of our own neural network library. The library is flexible enough to create an arbitrary number of sequential layers with different numbers of nodes and activation functions for each. The resulting network could be used for either binary classification (using a sigmoid activation function for the last layer), or regression (using a linear activation function for the last layer). The model is first trained by passing each row of the training data tensor through the network, producing an output after the final layer. This output is compared to the target value from the corresponding row in the training label tensor, and the weights of the network are updated using the backpropagation algorithm. The backpropagation algorithm updates each weight using the gradient of the error term with respect to the weight, and the magnitude of this update is controlled using the learning rate, which must be positive and has a maximum value of one. This process is repeated for the entire training data tensor, and for the number of epochs requested.\n",
    "\n",
    "Overall, the results of this assignment were very encouraging. Given the skeleton of the library, it took only a few hours to build up a functional program for creating our own neural networks. As with the Keras example from last week, the network does not achieve 100% accuracy on the XOR data every time it is trained. I thought it would be worth exploring the convergence rate, and ran the series of trials described above. The fact that the convergence rate for 100% accuracy is typically 70-80% is encouraging, but a higher rate (ideally over 90%) would be ideal. After conducting a bit of research, it seems the best way to increase the rate is to either add nodes to the single hidden layer, or add another layer all together. This seems to have the intended effect of pushing the convergence rate to over 90% in a lot of cases. I tried adding a third hidden node, and the convergence rate jumped to 98%, which is quite remarkable. I imagine these types of exercises will be common place once we start building more complex networks, and there isn't a preset formula for how the network should be constructed. Nevertheless, the fact that we can create a network capable of separating nonlinear classification data with just a few dozen lines of code is very impressive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
